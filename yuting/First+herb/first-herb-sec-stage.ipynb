{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T08:23:26.075713Z",
     "iopub.status.busy": "2025-05-14T08:23:26.075171Z",
     "iopub.status.idle": "2025-05-14T08:23:35.315347Z",
     "shell.execute_reply": "2025-05-14T08:23:35.315811Z",
     "shell.execute_reply.started": "2025-05-14T08:20:11.254844Z"
    },
    "papermill": {
     "duration": 9.260118,
     "end_time": "2025-05-14T08:23:35.316127",
     "exception": false,
     "start_time": "2025-05-14T08:23:26.056009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/timm-pytorch-image-model/pytorch-image-models-master/timm/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import gc\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from typing import Union\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score\n",
    "from functools import partial\n",
    "from albumentations import (Compose, OneOf, Normalize, Resize, RandomResizedCrop, RandomCrop, CenterCrop, \n",
    "                            HorizontalFlip, VerticalFlip, Rotate, ShiftScaleRotate, Transpose)\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations import ImageOnlyTransform\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "sys.path.insert(0, '/kaggle/input/timm-pytorch-image-model/pytorch-image-models-master')\n",
    "# sys.path.append('../input/pytorch-image-models/pytorch-image-models-master')\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "# from torchvision.models.feature_extraction import create_feature_extractor\n",
    "print(timm.__file__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011046,
     "end_time": "2025-05-14T08:23:35.339066",
     "exception": false,
     "start_time": "2025-05-14T08:23:35.328020",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1st Place Solution \"Cassava Leaf Disease Classification\"\n",
    "\n",
    "This is the inference notebook of our final submission which scored ~91.3% on public and private leaderboard. We used an ensemble of four different models and stacked those models together using a mean approach.\n",
    "\n",
    "You can find the according training code in these notebooks:\n",
    "\n",
    "* [EfficientNet B4 (TPU Training)](https://www.kaggle.com/jannish/cassava-leaf-disease-efficientnetb4-tpu)\n",
    "* [ResNext50_32x4d (GPU Training)](https://www.kaggle.com/hiarsl/cassava-leaf-disease-resnext50)\n",
    "* [ViT (TPU Training)](https://www.kaggle.com/sebastiangnther/cassava-leaf-disease-vit-tpu-training)\n",
    "\n",
    "In order to find the final combination of all the models we tested, we iteratively tried different ensembles using this notebook:\n",
    "\n",
    "* [Ensembling by using OOF predictions](https://www.kaggle.com/jannish/cassava-leaf-disease-finding-final-ensembles)\n",
    "\n",
    "Our final submission first averaged the probabilities of the predicted classes of ViT and ResNext. This averaged probability vector was then merged with the predicted probabilities of EfficientnetB4 and MobileNet(CropNet) in a second stage. For this purpose, the values were simply summed up.\n",
    "\n",
    "Finally, we would like to thank all the Kagglers who posted their notebooks and gave valuable hints on which models to try!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T08:23:35.367035Z",
     "iopub.status.busy": "2025-05-14T08:23:35.366535Z",
     "iopub.status.idle": "2025-05-14T08:23:35.382192Z",
     "shell.execute_reply": "2025-05-14T08:23:35.381809Z",
     "shell.execute_reply.started": "2025-05-14T08:20:11.263228Z"
    },
    "papermill": {
     "duration": 0.031986,
     "end_time": "2025-05-14T08:23:35.382317",
     "exception": false,
     "start_time": "2025-05-14T08:23:35.350331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = \"/kaggle/input/cassava-leaf-disease-classification/\"\n",
    "image_path = path+\"test_images/\"\n",
    "\n",
    "IMAGE_SIZE = (512,512)\n",
    "submission_df = pd.read_csv('../input/cassava-leaf-disease-classification/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01072,
     "end_time": "2025-05-14T08:23:35.404405",
     "exception": false,
     "start_time": "2025-05-14T08:23:35.393685",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Used models in the final submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T08:23:35.431397Z",
     "iopub.status.busy": "2025-05-14T08:23:35.430892Z",
     "iopub.status.idle": "2025-05-14T08:23:35.433212Z",
     "shell.execute_reply": "2025-05-14T08:23:35.432847Z",
     "shell.execute_reply.started": "2025-05-14T08:21:25.116035Z"
    },
    "papermill": {
     "duration": 0.017821,
     "end_time": "2025-05-14T08:23:35.433320",
     "exception": false,
     "start_time": "2025-05-14T08:23:35.415499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We used this flag to test combinations using only TF.Keras models\n",
    "onlykeras = False\n",
    "        \n",
    "used_models_pytorch = {\"vit2020\": [f'../input/cassava-leaf-disease-1st-place-models/vit/vit_base_patch16_384_fold_{fold}.h5' for fold in [0,1,2,3,4]],\n",
    "                       \"resnext\": [f'../input/cassava-leaf-disease-1st-place-models/resnext50_32x4d/resnext50_32x4d_fold{fold}_best.pth' for fold in [0,1,2,3,4]],\n",
    "                       \"HERB\": [f'../input/5-weights/backup/fold_{fold}_best.pt' for fold in [1,2,3,4,5]]}\n",
    "\n",
    "used_models_keras = {\"mobilenet\": \"../input/cassava-leaf-disease-1st-place-models/cropnet_mobilenetv3/cropnet\",\n",
    "                     \"efficientnetb4\": \"../input/cassava-leaf-disease-1st-place-models/efficientnetb4/efficientnetb4_all_e14.h5\"}\n",
    "\n",
    "# We used this flag for testing different ensembling approaches\n",
    "stacked_mean = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010861,
     "end_time": "2025-05-14T08:23:35.455400",
     "exception": false,
     "start_time": "2025-05-14T08:23:35.444539",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ResNext50_32x4d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T08:23:35.567168Z",
     "iopub.status.busy": "2025-05-14T08:23:35.566552Z",
     "iopub.status.idle": "2025-05-14T08:23:46.982891Z",
     "shell.execute_reply": "2025-05-14T08:23:46.982409Z",
     "shell.execute_reply.started": "2025-05-14T08:21:27.164236Z"
    },
    "papermill": {
     "duration": 11.515355,
     "end_time": "2025-05-14T08:23:46.983022",
     "exception": false,
     "start_time": "2025-05-14T08:23:35.467667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomResNext(nn.Module):\n",
    "        def __init__(self, model_name='resnext50_32x4d', pretrained=False):\n",
    "            super().__init__()\n",
    "            self.model = timm.create_model(model_name, pretrained=pretrained)\n",
    "            n_features = self.model.fc.in_features\n",
    "            self.model.fc = nn.Linear(n_features, 5)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.model(x)\n",
    "            return x\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.file_names = df['image_path_id'].values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_names[idx]\n",
    "        image = cv2.imread(file_name)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        return image\n",
    "\n",
    "if \"resnext\" in used_models_pytorch:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def get_transforms():\n",
    "        return Compose([Resize(512, 512),\n",
    "                        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                        ToTensorV2()])\n",
    "\n",
    "    def inference(model, states, test_loader, device):\n",
    "        model.to(device)\n",
    "\n",
    "        probabilities = []\n",
    "        for i, (images) in enumerate(test_loader):\n",
    "            images = images.to(device)\n",
    "            avg_preds = []\n",
    "            for state in states:\n",
    "                model.load_state_dict(state['model'])\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    y_preds = model(images)\n",
    "                avg_preds.append(y_preds.softmax(1).to('cpu').numpy())\n",
    "            avg_preds = np.mean(avg_preds, axis=0)\n",
    "            probabilities.append(avg_preds)\n",
    "        return np.concatenate(probabilities)\n",
    "    \n",
    "\n",
    "    predictions_resnext = pd.DataFrame(columns={\"image_id\"})\n",
    "    predictions_resnext[\"image_id\"] = submission_df[\"image_id\"].values\n",
    "    predictions_resnext['image_path_id'] = image_path + predictions_resnext['image_id'].astype(str)\n",
    "\n",
    "    model = CustomResNext('resnext50_32x4d', pretrained=False)\n",
    "    states = [torch.load(f) for f in used_models_pytorch[\"resnext\"]]\n",
    "\n",
    "    test_dataset = TestDataset(predictions_resnext, transform=get_transforms())\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    predictions = inference(model, states, test_loader, device)\n",
    "\n",
    "    predictions_resnext['resnext'] = [np.squeeze(p) for p in predictions]\n",
    "    predictions_resnext = predictions_resnext.drop([\"image_path_id\"], axis=1)\n",
    "    \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    try:\n",
    "        del(model)\n",
    "        del(states)\n",
    "    except:\n",
    "        pass\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.0109,
     "end_time": "2025-05-14T08:23:47.005400",
     "exception": false,
     "start_time": "2025-05-14T08:23:46.994500",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# HERB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T08:23:47.034218Z",
     "iopub.status.busy": "2025-05-14T08:23:47.033626Z",
     "iopub.status.idle": "2025-05-14T08:23:47.036085Z",
     "shell.execute_reply": "2025-05-14T08:23:47.035612Z",
     "shell.execute_reply.started": "2025-05-14T08:21:33.870257Z"
    },
    "papermill": {
     "duration": 0.019788,
     "end_time": "2025-05-14T08:23:47.036191",
     "exception": false,
     "start_time": "2025-05-14T08:23:47.016403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_transforms = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((510, 510), Image.BILINEAR),\n",
    "        transforms.CenterCrop((384, 384)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "])\n",
    "\n",
    "\n",
    "class HERBDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.file_names = df['image_path_id'].values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_names[idx]\n",
    "        image = cv2.imread(file_name)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T08:23:47.109357Z",
     "iopub.status.busy": "2025-05-14T08:23:47.066642Z",
     "iopub.status.idle": "2025-05-14T08:23:47.114968Z",
     "shell.execute_reply": "2025-05-14T08:23:47.114579Z",
     "shell.execute_reply.started": "2025-05-14T08:20:13.354678Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.067861,
     "end_time": "2025-05-14T08:23:47.115097",
     "exception": false,
     "start_time": "2025-05-14T08:23:47.047236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GCNCombiner(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 total_num_selects: int,\n",
    "                 num_classes: int, \n",
    "                 inputs: Union[dict, None] = None, \n",
    "                 proj_size: Union[int, None] = None,\n",
    "                 fpn_size: Union[int, None] = None):\n",
    "        \"\"\"\n",
    "        If building backbone without FPN, set fpn_size to None and MUST give \n",
    "        'inputs' and 'proj_size', the reason of these setting is to constrain the \n",
    "        dimension of graph convolutional network input.\n",
    "        \"\"\"\n",
    "        super(GCNCombiner, self).__init__()\n",
    "\n",
    "        assert inputs is not None or fpn_size is not None, \\\n",
    "            \"To build GCN combiner, you must give one features dimension.\"\n",
    "\n",
    "        ### auto-proj\n",
    "        self.fpn_size = fpn_size\n",
    "        if fpn_size is None:\n",
    "            for name in inputs:\n",
    "                if len(name) == 4:\n",
    "                    in_size = inputs[name].size(1)\n",
    "                elif len(name) == 3:\n",
    "                    in_size = inputs[name].size(2)\n",
    "                else:\n",
    "                    raise ValueError(\"The size of output dimension of previous must be 3 or 4.\")\n",
    "                m = nn.Sequential(\n",
    "                    nn.Linear(in_size, proj_size),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(proj_size, proj_size)\n",
    "                )\n",
    "                self.add_module(\"proj_\"+name, m)\n",
    "            self.proj_size = proj_size\n",
    "        else:\n",
    "            self.proj_size = fpn_size\n",
    "\n",
    "        ### build one layer structure (with adaptive module)\n",
    "        num_joints = total_num_selects // 64\n",
    "\n",
    "        self.param_pool0 = nn.Linear(total_num_selects, num_joints)\n",
    "        \n",
    "        A = torch.eye(num_joints) / 100 + 1 / 100\n",
    "        self.adj1 = nn.Parameter(copy.deepcopy(A))\n",
    "        self.conv1 = nn.Conv1d(self.proj_size, self.proj_size, 1)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(self.proj_size)\n",
    "        \n",
    "        self.conv_q1 = nn.Conv1d(self.proj_size, self.proj_size//4, 1)\n",
    "        self.conv_k1 = nn.Conv1d(self.proj_size, self.proj_size//4, 1)\n",
    "        self.alpha1 = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        ### merge information\n",
    "        self.param_pool1 = nn.Linear(num_joints, 1)\n",
    "        \n",
    "        #### class predict\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.classifier = nn.Linear(self.proj_size, num_classes)\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        hs = []\n",
    "        names = []\n",
    "        for name in x:\n",
    "            if \"FPN1_\" in name:\n",
    "                continue\n",
    "            if self.fpn_size is None:\n",
    "                _tmp = getattr(self, \"proj_\"+name)(x[name])\n",
    "            else:\n",
    "                _tmp = x[name]\n",
    "            hs.append(_tmp)\n",
    "            names.append([name, _tmp.size()])\n",
    "\n",
    "        hs = torch.cat(hs, dim=1).transpose(1, 2).contiguous() # B, S', C --> B, C, S\n",
    "        hs = self.param_pool0(hs)\n",
    "        ### adaptive adjacency\n",
    "        q1 = self.conv_q1(hs).mean(1)\n",
    "        k1 = self.conv_k1(hs).mean(1)\n",
    "        A1 = self.tanh(q1.unsqueeze(-1) - k1.unsqueeze(1))\n",
    "        A1 = self.adj1 + A1 * self.alpha1\n",
    "        ### graph convolution\n",
    "        hs = self.conv1(hs)\n",
    "        hs = torch.matmul(hs, A1)\n",
    "        hs = self.batch_norm1(hs)\n",
    "        ### predict\n",
    "        hs = self.param_pool1(hs)\n",
    "        hs = self.dropout(hs)\n",
    "        hs = hs.flatten(1)\n",
    "        hs = self.classifier(hs)\n",
    "\n",
    "        return hs\n",
    "    \n",
    "class WeaklySelector(nn.Module):\n",
    "\n",
    "    def __init__(self, inputs: dict, num_classes: int, num_select: dict, fpn_size: Union[int, None] = None):\n",
    "        \"\"\"\n",
    "        inputs: dictionary contain torch.Tensors, which comes from backbone\n",
    "                [Tensor1(hidden feature1), Tensor2(hidden feature2)...]\n",
    "                Please note that if len(features.size) equal to 3, the order of dimension must be [B,S,C],\n",
    "                S mean the spatial domain, and if len(features.size) equal to 4, the order must be [B,C,H,W]\n",
    "        \"\"\"\n",
    "        super(WeaklySelector, self).__init__()\n",
    "\n",
    "        self.num_select = num_select\n",
    "\n",
    "        self.fpn_size = fpn_size\n",
    "        ### build classifier\n",
    "        if self.fpn_size is None:\n",
    "            self.num_classes = num_classes\n",
    "            for name in inputs:\n",
    "                fs_size = inputs[name].size()\n",
    "                if len(fs_size) == 3:\n",
    "                    in_size = fs_size[2]\n",
    "                elif len(fs_size) == 4:\n",
    "                    in_size = fs_size[1]\n",
    "                m = nn.Linear(in_size, num_classes)\n",
    "                self.add_module(\"classifier_l_\"+name, m)\n",
    "\n",
    "        self.thresholds = {}\n",
    "        for name in inputs:\n",
    "            self.thresholds[name] = []\n",
    "\n",
    "    # def select(self, logits, l_name):\n",
    "    #     \"\"\"\n",
    "    #     logits: [B, S, num_classes]\n",
    "    #     \"\"\"\n",
    "    #     probs = torch.softmax(logits, dim=-1)\n",
    "    #     scores, _ = torch.max(probs, dim=-1)\n",
    "    #     _, ids = torch.sort(scores, -1, descending=True)\n",
    "    #     sn = self.num_select[l_name]\n",
    "    #     s_ids = ids[:, :sn]\n",
    "    #     not_s_ids = ids[:, sn:]\n",
    "    #     return s_ids.unsqueeze(-1), not_s_ids.unsqueeze(-1)\n",
    "\n",
    "    def forward(self, x, logits=None):\n",
    "        \"\"\"\n",
    "        x : \n",
    "            dictionary contain the features maps which \n",
    "            come from your choosen layers.\n",
    "            size must be [B, HxW, C] ([B, S, C]) or [B, C, H, W].\n",
    "            [B,C,H,W] will be transpose to [B, HxW, C] automatically.\n",
    "        \"\"\"\n",
    "        if self.fpn_size is None:\n",
    "            logits = {}\n",
    "        selections = {}\n",
    "        for name in x:\n",
    "            if \"FPN1_\" in name:\n",
    "                continue\n",
    "            if len(x[name].size()) == 4:\n",
    "                B, C, H, W = x[name].size()\n",
    "                x[name] = x[name].view(B, C, H*W).permute(0, 2, 1).contiguous()\n",
    "            C = x[name].size(-1)\n",
    "            if self.fpn_size is None:\n",
    "                logits[name] = getattr(self, \"classifier_l_\"+name)(x[name])\n",
    "            \n",
    "            probs = torch.softmax(logits[name], dim=-1)\n",
    "            sum_probs = torch.softmax(logits[name].mean(1), dim=-1)\n",
    "            selections[name] = []\n",
    "            preds_1 = []\n",
    "            preds_0 = []\n",
    "            num_select = self.num_select[name]\n",
    "            for bi in range(logits[name].size(0)):\n",
    "                _, max_ids = torch.max(sum_probs[bi], dim=-1)\n",
    "                confs, ranks = torch.sort(probs[bi, :, max_ids], descending=True)\n",
    "                sf = x[name][bi][ranks[:num_select]]\n",
    "                nf = x[name][bi][ranks[num_select:]]  # calculate\n",
    "                selections[name].append(sf) # [num_selected, C]\n",
    "                preds_1.append(logits[name][bi][ranks[:num_select]])\n",
    "                preds_0.append(logits[name][bi][ranks[num_select:]])\n",
    "\n",
    "                if bi >= len(self.thresholds[name]):\n",
    "                    self.thresholds[name].append(confs[num_select]) # for initialize\n",
    "                else:\n",
    "                    self.thresholds[name][bi] = confs[num_select]\n",
    "            \n",
    "            selections[name] = torch.stack(selections[name])\n",
    "            preds_1 = torch.stack(preds_1)\n",
    "            preds_0 = torch.stack(preds_0)\n",
    "\n",
    "            logits[\"select_\"+name] = preds_1\n",
    "            logits[\"drop_\"+name] = preds_0\n",
    "\n",
    "        return selections\n",
    "    \n",
    "class FPN(nn.Module):\n",
    "\n",
    "    def __init__(self, inputs: dict, fpn_size: int, proj_type: str, upsample_type: str):\n",
    "        \"\"\"\n",
    "        inputs : dictionary contains torch.Tensor\n",
    "                 which comes from backbone output\n",
    "        fpn_size: integer, fpn \n",
    "        proj_type: \n",
    "            in [\"Conv\", \"Linear\"]\n",
    "        upsample_type:\n",
    "            in [\"Bilinear\", \"Conv\", \"Fc\"]\n",
    "            for convolution neural network (e.g. ResNet, EfficientNet), recommand 'Bilinear'. \n",
    "            for Vit, \"Fc\". and Swin-T, \"Conv\"\n",
    "        \"\"\"\n",
    "        super(FPN, self).__init__()\n",
    "        assert proj_type in [\"Conv\", \"Linear\"], \\\n",
    "            \"FPN projection type {} were not support yet, please choose type 'Conv' or 'Linear'\".format(proj_type)\n",
    "        assert upsample_type in [\"Bilinear\", \"Conv\"], \\\n",
    "            \"FPN upsample type {} were not support yet, please choose type 'Bilinear' or 'Conv'\".format(proj_type)\n",
    "\n",
    "        self.fpn_size = fpn_size\n",
    "        self.upsample_type = upsample_type\n",
    "        inp_names = [name for name in inputs]\n",
    "        for i, node_name in enumerate(inputs):\n",
    "            ### projection module\n",
    "            if proj_type == \"Conv\":\n",
    "                m = nn.Sequential(\n",
    "                    nn.Conv2d(inputs[node_name].size(1), inputs[node_name].size(1), 1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv2d(inputs[node_name].size(1), fpn_size, 1)\n",
    "                )\n",
    "            elif proj_type == \"Linear\":\n",
    "                m = nn.Sequential(\n",
    "                    nn.Linear(inputs[node_name].size(-1), inputs[node_name].size(-1)),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(inputs[node_name].size(-1), fpn_size),\n",
    "                )\n",
    "            self.add_module(\"Proj_\"+node_name, m)\n",
    "\n",
    "            ### upsample module\n",
    "            if upsample_type == \"Conv\" and i != 0:\n",
    "                assert len(inputs[node_name].size()) == 3 # B, S, C\n",
    "                in_dim = inputs[node_name].size(1)\n",
    "                out_dim = inputs[inp_names[i-1]].size(1)\n",
    "                # if in_dim != out_dim:\n",
    "                m = nn.Conv1d(in_dim, out_dim, 1) # for spatial domain\n",
    "                # else:\n",
    "                #     m = nn.Identity()\n",
    "                self.add_module(\"Up_\"+node_name, m)\n",
    "\n",
    "        if upsample_type == \"Bilinear\":\n",
    "            self.upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "\n",
    "    def upsample_add(self, x0: torch.Tensor, x1: torch.Tensor, x1_name: str):\n",
    "        \"\"\"\n",
    "        return Upsample(x1) + x1\n",
    "        \"\"\"\n",
    "        if self.upsample_type == \"Bilinear\":\n",
    "            if x1.size(-1) != x0.size(-1):\n",
    "                x1 = self.upsample(x1)\n",
    "        else:\n",
    "            x1 = getattr(self, \"Up_\"+x1_name)(x1)\n",
    "        return x1 + x0\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x : dictionary\n",
    "            {\n",
    "                \"node_name1\": feature1,\n",
    "                \"node_name2\": feature2, ...\n",
    "            }\n",
    "        \"\"\"\n",
    "        ### project to same dimension\n",
    "        hs = []\n",
    "        for i, name in enumerate(x):\n",
    "            if \"FPN1_\" in name:\n",
    "                continue\n",
    "            x[name] = getattr(self, \"Proj_\"+name)(x[name])\n",
    "            hs.append(name)\n",
    "        \n",
    "        x[\"FPN1_\" + \"layer4\"] = x[\"layer4\"]\n",
    "\n",
    "        for i in range(len(hs)-1, 0, -1):\n",
    "            x1_name = hs[i]\n",
    "            x0_name = hs[i-1]\n",
    "            x[x0_name] = self.upsample_add(x[x0_name], \n",
    "                                           x[x1_name], \n",
    "                                           x1_name)\n",
    "            x[\"FPN1_\" + x0_name] = x[x0_name]\n",
    "\n",
    "        return x\n",
    "\n",
    "class FPN_UP(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 inputs: dict, \n",
    "                 fpn_size: int):\n",
    "        super(FPN_UP, self).__init__()\n",
    "\n",
    "        inp_names = [name for name in inputs]\n",
    "\n",
    "        for i, node_name in enumerate(inputs):\n",
    "            ### projection module\n",
    "            m = nn.Sequential(\n",
    "                nn.Linear(fpn_size, fpn_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fpn_size, fpn_size),\n",
    "            )\n",
    "            self.add_module(\"Proj_\"+node_name, m)\n",
    "\n",
    "            ### upsample module\n",
    "            if i != (len(inputs) - 1):\n",
    "                assert len(inputs[node_name].size()) == 3 # B, S, C\n",
    "                in_dim = inputs[node_name].size(1)\n",
    "                out_dim = inputs[inp_names[i+1]].size(1)\n",
    "                m = nn.Conv1d(in_dim, out_dim, 1) # for spatial domain\n",
    "                self.add_module(\"Down_\"+node_name, m)\n",
    "                \"\"\"\n",
    "                Down_layer1 2304 576\n",
    "                Down_layer2 576 144\n",
    "                Down_layer3 144 144\n",
    "                \"\"\"\n",
    "\n",
    "    def downsample_add(self, x0: torch.Tensor, x1: torch.Tensor, x0_name: str):\n",
    "        \"\"\"\n",
    "        return Upsample(x1) + x1\n",
    "        \"\"\"\n",
    "        # print(\"[downsample_add] Down_\" + x0_name)\n",
    "        x0 = getattr(self, \"Down_\" + x0_name)(x0)\n",
    "        return x1 + x0\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x : dictionary\n",
    "            {\n",
    "                \"node_name1\": feature1,\n",
    "                \"node_name2\": feature2, ...\n",
    "            }\n",
    "        \"\"\"\n",
    "        ### project to same dimension\n",
    "        hs = []\n",
    "        for i, name in enumerate(x):\n",
    "            if \"FPN1_\" in name:\n",
    "                continue\n",
    "            x[name] = getattr(self, \"Proj_\"+name)(x[name])\n",
    "            hs.append(name)\n",
    "\n",
    "        # print(hs)\n",
    "        for i in range(0, len(hs) - 1):\n",
    "            x0_name = hs[i]\n",
    "            x1_name = hs[i+1]\n",
    "            # print(x0_name, x1_name)\n",
    "            # print(x[x0_name].size(), x[x1_name].size())\n",
    "            x[x1_name] = self.downsample_add(x[x0_name], \n",
    "                                             x[x1_name], \n",
    "                                             x0_name)\n",
    "        return x\n",
    "\n",
    "class PluginMoodel(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 backbone: torch.nn.Module,\n",
    "                 return_nodes: Union[dict, None],\n",
    "                 img_size: int,\n",
    "                 use_fpn: bool,\n",
    "                 fpn_size: Union[int, None],\n",
    "                 proj_type: str,\n",
    "                 upsample_type: str,\n",
    "                 use_selection: bool,\n",
    "                 num_classes: int,\n",
    "                 num_selects: dict, \n",
    "                 use_combiner: bool,\n",
    "                 comb_proj_size: Union[int, None]\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        * backbone: \n",
    "            torch.nn.Module class (recommand pretrained on ImageNet or IG-3.5B-17k(provided by FAIR))\n",
    "        * return_nodes:\n",
    "            e.g.\n",
    "            return_nodes = {\n",
    "                # node_name: user-specified key for output dict\n",
    "                'layer1.2.relu_2': 'layer1',\n",
    "                'layer2.3.relu_2': 'layer2',\n",
    "                'layer3.5.relu_2': 'layer3',\n",
    "                'layer4.2.relu_2': 'layer4',\n",
    "            } # you can see the example on https://pytorch.org/vision/main/feature_extraction.html\n",
    "            !!! if using 'Swin-Transformer', please set return_nodes to None\n",
    "            !!! and please set use_fpn to True\n",
    "        * feat_sizes: \n",
    "            tuple or list contain features map size of each layers. \n",
    "            ((C, H, W)). e.g. ((1024, 14, 14), (2048, 7, 7))\n",
    "        * use_fpn: \n",
    "            boolean, use features pyramid network or not\n",
    "        * fpn_size: \n",
    "            integer, features pyramid network projection dimension\n",
    "        * num_selects:\n",
    "            num_selects = {\n",
    "                # match user-specified in return_nodes\n",
    "                \"layer1\": 2048,\n",
    "                \"layer2\": 512,\n",
    "                \"layer3\": 128,\n",
    "                \"layer4\": 32,\n",
    "            }\n",
    "        Note: after selector module (WeaklySelector) , the feature map's size is [B, S', C] which \n",
    "        contained by 'logits' or 'selections' dictionary (S' is selection number, different layer \n",
    "        could be different).\n",
    "        \"\"\"\n",
    "        super(PluginMoodel, self).__init__()\n",
    "        \n",
    "        ### = = = = = Backbone = = = = =\n",
    "        self.return_nodes = return_nodes\n",
    "        \n",
    "        self.backbone = backbone\n",
    "        \n",
    "        ### get hidden feartues size\n",
    "        rand_in = torch.randn(1, 3, img_size, img_size)\n",
    "        outs = self.backbone(rand_in)\n",
    "        # print('outs')\n",
    "        # print(outs)\n",
    "\n",
    "        ### just original backbone\n",
    "        if not use_fpn and (not use_selection and not use_combiner):\n",
    "            for name in outs:\n",
    "                fs_size = outs[name].size()\n",
    "                if len(fs_size) == 3:\n",
    "                    out_size = fs_size.size(-1)\n",
    "                elif len(fs_size) == 4:\n",
    "                    out_size = fs_size.size(1)\n",
    "                else:\n",
    "                    raise ValueError(\"The size of output dimension of previous must be 3 or 4.\")\n",
    "            self.classifier = nn.Linear(out_size, num_classes)\n",
    "\n",
    "        ### = = = = = FPN = = = = =\n",
    "        self.use_fpn = use_fpn\n",
    "        if self.use_fpn:\n",
    "            self.fpn_down = FPN(outs, fpn_size, proj_type, upsample_type)\n",
    "            self.build_fpn_classifier_down(outs, fpn_size, num_classes)\n",
    "            self.fpn_up = FPN_UP(outs, fpn_size)\n",
    "            self.build_fpn_classifier_up(outs, fpn_size, num_classes)\n",
    "\n",
    "        self.fpn_size = fpn_size\n",
    "\n",
    "        ### = = = = = Selector = = = = =\n",
    "        self.use_selection = use_selection\n",
    "        if self.use_selection:\n",
    "            w_fpn_size = self.fpn_size if self.use_fpn else None # if not using fpn, build classifier in weakly selector\n",
    "            self.selector = WeaklySelector(outs, num_classes, num_selects, w_fpn_size)\n",
    "\n",
    "        ### = = = = = Combiner = = = = =\n",
    "        self.use_combiner = use_combiner\n",
    "        if self.use_combiner:\n",
    "            assert self.use_selection, \"Please use selection module before combiner\"\n",
    "            if self.use_fpn:\n",
    "                gcn_inputs, gcn_proj_size = None, None\n",
    "            else:\n",
    "                gcn_inputs, gcn_proj_size = outs, comb_proj_size # redundant, fix in future\n",
    "            total_num_selects = sum([num_selects[name] for name in num_selects]) # sum\n",
    "            self.combiner = GCNCombiner(total_num_selects, num_classes, gcn_inputs, gcn_proj_size, self.fpn_size)\n",
    "\n",
    "    def build_fpn_classifier_up(self, inputs: dict, fpn_size: int, num_classes: int):\n",
    "        \"\"\"\n",
    "        Teh results of our experiments show that linear classifier in this case may cause some problem.\n",
    "        \"\"\"\n",
    "        for name in inputs:\n",
    "            m = nn.Sequential(\n",
    "                    nn.Conv1d(fpn_size, fpn_size, 1),\n",
    "                    nn.BatchNorm1d(fpn_size),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv1d(fpn_size, num_classes, 1)\n",
    "                )\n",
    "            self.add_module(\"fpn_classifier_up_\"+name, m)\n",
    "\n",
    "    def build_fpn_classifier_down(self, inputs: dict, fpn_size: int, num_classes: int):\n",
    "        \"\"\"\n",
    "        Teh results of our experiments show that linear classifier in this case may cause some problem.\n",
    "        \"\"\"\n",
    "        for name in inputs:\n",
    "            m = nn.Sequential(\n",
    "                    nn.Conv1d(fpn_size, fpn_size, 1),\n",
    "                    nn.BatchNorm1d(fpn_size),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv1d(fpn_size, num_classes, 1)\n",
    "                )\n",
    "            self.add_module(\"fpn_classifier_down_\" + name, m)\n",
    "\n",
    "    def forward_backbone(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "    def fpn_predict_down(self, x: dict, logits: dict):\n",
    "        \"\"\"\n",
    "        x: [B, C, H, W] or [B, S, C]\n",
    "           [B, C, H, W] --> [B, H*W, C]\n",
    "        \"\"\"\n",
    "        for name in x:\n",
    "            if \"FPN1_\" not in name:\n",
    "                continue \n",
    "            ### predict on each features point\n",
    "            if len(x[name].size()) == 4:\n",
    "                B, C, H, W = x[name].size()\n",
    "                logit = x[name].view(B, C, H*W)\n",
    "            elif len(x[name].size()) == 3:\n",
    "                logit = x[name].transpose(1, 2).contiguous()\n",
    "            model_name = name.replace(\"FPN1_\", \"\")\n",
    "            logits[name] = getattr(self, \"fpn_classifier_down_\" + model_name)(logit)\n",
    "            logits[name] = logits[name].transpose(1, 2).contiguous() # transpose\n",
    "\n",
    "    def fpn_predict_up(self, x: dict, logits: dict):\n",
    "        \"\"\"\n",
    "        x: [B, C, H, W] or [B, S, C]\n",
    "           [B, C, H, W] --> [B, H*W, C]\n",
    "        \"\"\"\n",
    "        for name in x:\n",
    "            if \"FPN1_\" in name:\n",
    "                continue\n",
    "            ### predict on each features point\n",
    "            if len(x[name].size()) == 4:\n",
    "                B, C, H, W = x[name].size()\n",
    "                logit = x[name].view(B, C, H*W)\n",
    "            elif len(x[name].size()) == 3:\n",
    "                logit = x[name].transpose(1, 2).contiguous()\n",
    "            model_name = name.replace(\"FPN1_\", \"\")\n",
    "            logits[name] = getattr(self, \"fpn_classifier_up_\" + model_name)(logit)\n",
    "            logits[name] = logits[name].transpose(1, 2).contiguous() # transpose\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "\n",
    "        logits = {}\n",
    "\n",
    "        x = self.forward_backbone(x)\n",
    "\n",
    "        if self.use_fpn:\n",
    "            x = self.fpn_down(x)\n",
    "            # print([name for name in x])\n",
    "            self.fpn_predict_down(x, logits)\n",
    "            x = self.fpn_up(x)\n",
    "            self.fpn_predict_up(x, logits)\n",
    "\n",
    "        if self.use_selection:\n",
    "            selects = self.selector(x, logits)\n",
    "\n",
    "        if self.use_combiner:\n",
    "            comb_outs = self.combiner(selects)\n",
    "            logits['comb_outs'] = comb_outs\n",
    "            return logits\n",
    "        \n",
    "        if self.use_selection or self.fpn:\n",
    "            return logits\n",
    "\n",
    "        ### original backbone (only predict final selected layer)\n",
    "        for name in x:\n",
    "            hs = x[name]\n",
    "\n",
    "        if len(hs.size()) == 4:\n",
    "            hs = F.adaptive_avg_pool2d(hs, (1, 1))\n",
    "            hs = hs.flatten(1)\n",
    "        else:\n",
    "            hs = hs.mean(1)\n",
    "        out = self.classifier(hs)\n",
    "        logits['ori_out'] = logits\n",
    "\n",
    "        return logits\n",
    "\n",
    "@torch.no_grad()\n",
    "def sum_all_out(out, sum_type=\"softmax\"):\n",
    "    target_layer_names = \\\n",
    "    ['layer1', 'layer2', 'layer3', 'layer4',\n",
    "    'FPN1_layer1', 'FPN1_layer2', 'FPN1_layer3', 'FPN1_layer4', \n",
    "    'comb_outs']\n",
    "    sum_out = None\n",
    "    total = 0\n",
    "    for name in target_layer_names:\n",
    "        if name != \"comb_outs\":\n",
    "            tmp_out = out[name].mean(1)\n",
    "        else:\n",
    "            tmp_out = out[name]\n",
    "        \n",
    "        if sum_type == \"softmax\":\n",
    "            tmp_out = torch.softmax(tmp_out, dim=-1)\n",
    "            total += 1\n",
    "        if sum_out is None:\n",
    "            sum_out = tmp_out\n",
    "        else:\n",
    "            sum_out = sum_out + tmp_out # note that use '+=' would cause inplace error\n",
    "    return sum_out / total\n",
    "\n",
    "def build_model(pretrainewd_path: str,\n",
    "                img_size: int, \n",
    "                fpn_size: int, \n",
    "                num_classes: int,\n",
    "                num_selects: dict,\n",
    "                use_fpn: bool = True, \n",
    "                use_selection: bool = True,\n",
    "                use_combiner: bool = True, \n",
    "                comb_proj_size: int = None):\n",
    "    model = \\\n",
    "        PluginMoodel(backbone=timm.create_model('swin_large_patch4_window12_384_in22k'),\n",
    "                     return_nodes=None,\n",
    "                     img_size = img_size,\n",
    "                     use_fpn = use_fpn,\n",
    "                     fpn_size = fpn_size,\n",
    "                     proj_type = \"Linear\",\n",
    "                     upsample_type = \"Conv\",\n",
    "                     use_selection = use_selection,\n",
    "                     num_classes = num_classes,\n",
    "                     num_selects = num_selects, \n",
    "                     use_combiner = use_combiner,\n",
    "                     comb_proj_size = comb_proj_size)\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T08:23:47.150336Z",
     "iopub.status.busy": "2025-05-14T08:23:47.149827Z",
     "iopub.status.idle": "2025-05-14T08:24:46.743212Z",
     "shell.execute_reply": "2025-05-14T08:24:46.743655Z",
     "shell.execute_reply.started": "2025-05-14T08:21:40.279536Z"
    },
    "papermill": {
     "duration": 59.617252,
     "end_time": "2025-05-14T08:24:46.743824",
     "exception": false,
     "start_time": "2025-05-14T08:23:47.126572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "if \"HERB\" in used_models_pytorch:\n",
    "    device = (\n",
    "        \"cuda\"\n",
    "        if torch.cuda.is_available()\n",
    "        else \"mps\"\n",
    "        if torch.backends.mps.is_available()\n",
    "        else \"cpu\"\n",
    "    )\n",
    "    print(f\"Using {device} device\")\n",
    "    def inference(model, states, test_loader, device):\n",
    "            model.to(device)\n",
    "    \n",
    "            probabilities = []\n",
    "            for i, (images) in enumerate(test_loader):\n",
    "                images = images.to(device)\n",
    "                avg_preds = []\n",
    "                for state in states:\n",
    "                    model.load_state_dict(state['model'])\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        outs = model(images)\n",
    "                        outs = sum_all_out(outs, sum_type=\"softmax\") # softmax\n",
    "                        # print(outs.shape)\n",
    "                    avg_preds.append(outs.to('cpu').numpy())\n",
    "                avg_preds = np.mean(avg_preds, axis=0)\n",
    "                probabilities.append(avg_preds)\n",
    "            return np.concatenate(probabilities)  \n",
    "    \n",
    "    predictions_herb = pd.DataFrame(columns={\"image_id\"})\n",
    "    predictions_herb[\"image_id\"] = submission_df[\"image_id\"].values\n",
    "    predictions_herb['image_path_id'] = image_path + predictions_herb['image_id'].astype(str)\n",
    "    \n",
    "    model = build_model(\n",
    "        pretrainewd_path='',\n",
    "        img_size=384,\n",
    "        fpn_size=1536,\n",
    "        num_classes=5,\n",
    "        num_selects={'layer1': 256, 'layer2': 128, 'layer3': 64,'layer4': 32}\n",
    "    )\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    states = [torch.load(f, map_location=device) for f in used_models_pytorch[\"HERB\"]]\n",
    "    \n",
    "    # 4. Predict on test data\n",
    "    test_dataset = HERBDataset(predictions_herb, transform=val_transforms)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    # myDataloader = DataLoader(myCassavaDataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "    \n",
    "    # Predict on test data\n",
    "    predictions = inference(model, states, test_loader, device)\n",
    "    \n",
    "    predictions_herb['herb'] = [np.squeeze(p) for p in predictions]\n",
    "    predictions_herb = predictions_herb.drop([\"image_path_id\"], axis=1)\n",
    "    \n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    try:\n",
    "        del(model)\n",
    "        del(states)\n",
    "    except:\n",
    "        pass\n",
    "    gc.collect()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01215,
     "end_time": "2025-05-14T08:24:46.769640",
     "exception": false,
     "start_time": "2025-05-14T08:24:46.757490",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T08:24:46.808706Z",
     "iopub.status.busy": "2025-05-14T08:24:46.800172Z",
     "iopub.status.idle": "2025-05-14T08:25:04.362546Z",
     "shell.execute_reply": "2025-05-14T08:25:04.362925Z"
    },
    "papermill": {
     "duration": 17.581983,
     "end_time": "2025-05-14T08:25:04.363084",
     "exception": false,
     "start_time": "2025-05-14T08:24:46.781101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.22it/s]\n"
     ]
    }
   ],
   "source": [
    "if \"vit2020\" in used_models_pytorch:\n",
    "    \n",
    "    vit_image_size = 384\n",
    "    \n",
    "    class CustomViT(nn.Module):\n",
    "        def __init__(self, model_arch, n_class, pretrained=False):\n",
    "            super().__init__()\n",
    "            self.model = timm.create_model(model_arch, pretrained=pretrained)\n",
    "            n_features = self.model.head.in_features\n",
    "            self.model.head = nn.Linear(n_features, n_class)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.model(x)\n",
    "            return x\n",
    "        \n",
    "    class TestDataset(Dataset):\n",
    "        def __init__(self, df, transform=None):\n",
    "            self.df = df\n",
    "            self.file_names = df['image_path_id'].values\n",
    "            self.transform = transform\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.df)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            file_name = self.file_names[idx]\n",
    "            im_bgr = cv2.imread(file_name)\n",
    "            image = im_bgr[:, :, ::-1]\n",
    "            if self.transform:\n",
    "                augmented = self.transform(image=image)\n",
    "                image = augmented['image']\n",
    "            return image\n",
    "        \n",
    "\n",
    "    def get_tta_transforms():\n",
    "        return Compose([CenterCrop(vit_image_size, vit_image_size, p=1.),\n",
    "                Resize(vit_image_size, vit_image_size),\n",
    "                Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "                ToTensorV2(p=1.0)], p=1.)\n",
    "\n",
    "    def inference(models, test_loader, device):\n",
    "        tk0 = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "        probs = []\n",
    "        for i, (images) in tk0:\n",
    "            avg_preds = []\n",
    "            for model in models:\n",
    "                images = images.to(device)\n",
    "                model.to(device)\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    y_preds = model(images)\n",
    "                avg_preds.append(y_preds.softmax(1).to('cpu').numpy())\n",
    "            avg_preds = np.mean(avg_preds, axis=0)\n",
    "            probs.append(avg_preds)\n",
    "        probs = np.concatenate(probs)\n",
    "        return probs\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    predictions_vit = pd.DataFrame(columns={\"image_id\"})\n",
    "    predictions_vit[\"image_id\"] = submission_df[\"image_id\"].values\n",
    "    predictions_vit['image_path_id'] = image_path + predictions_vit['image_id'].astype(str)\n",
    "\n",
    "    def load_cassava_vit(modelpath):\n",
    "        _model = CustomViT('vit_base_patch16_384', 5, pretrained=False)\n",
    "        _model.load_state_dict(torch.load(modelpath))\n",
    "        _model.eval()\n",
    "        return _model\n",
    "\n",
    "    models = [load_cassava_vit(f) for f in used_models_pytorch[\"vit2020\"]]\n",
    "\n",
    "    test_dataset = TestDataset(predictions_vit, transform=get_tta_transforms())\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    predictions_raw_vit = inference(models, test_loader, device)\n",
    "\n",
    "    predictions_vit['vit2020'] = [np.squeeze(p) for p in predictions_raw_vit]\n",
    "    predictions_vit = predictions_vit.drop([\"image_path_id\"], axis=1)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    try:\n",
    "        for model in models:\n",
    "            del(model)\n",
    "    except:\n",
    "        pass\n",
    "    models = []\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012185,
     "end_time": "2025-05-14T08:25:04.388086",
     "exception": false,
     "start_time": "2025-05-14T08:25:04.375901",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Mobilenet V3 (CropNet)\n",
    "\n",
    "There are multiple ways to include pretrained models from [TensorFlow Hub](https://www.tensorflow.org/hub/tutorials/cropnet_cassava), if internet has to be turned of during submission:\n",
    "\n",
    "* Accessing and storing the .tar.gz file (see [this](https://xianbao-qian.medium.com/how-to-run-tf-hub-locally-without-internet-connection-4506b850a915) Medium post) \n",
    "<code>\n",
    "!curl -LO https://storage.googleapis.com/tfhub-modules/google/cropnet/classifier/cassava_disease_V1/2.tar.gz\n",
    "!mkdir cropnet_mobilenetv3\n",
    "!tar -xf 2.tar.gz  --directory cropnet_mobilenetv3    \n",
    "</code>\n",
    "<br>\n",
    "\n",
    "* Downloading and caching the weights using\n",
    "<code>\n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = \"/kaggle/working\"\n",
    "hub.KerasLayer('https://tfhub.dev/google/cropnet/classifier/cassava_disease_V1/2', trainable=False)\n",
    "</code>\n",
    "<br>\n",
    "\n",
    "You can find more [information on caching on the official tfhub website](https://www.tensorflow.org/hub/caching) and more information on the [pretrained CropNet model ](https://tfhub.dev/google/cropnet/classifier/cassava_disease_V1/2). For the offline submissions we included these weights into a Kaggle Dataset bucket.\n",
    "\n",
    "Remark: In the meantime, TFHub models can apparently be integrated directly into the TPU training via Kaggle. Check out the[ Kaggle TPU FAQs](https://www.kaggle.com/product-feedback/216256)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T08:25:04.417884Z",
     "iopub.status.busy": "2025-05-14T08:25:04.417144Z",
     "iopub.status.idle": "2025-05-14T08:25:04.563471Z",
     "shell.execute_reply": "2025-05-14T08:25:04.564344Z"
    },
    "papermill": {
     "duration": 0.164106,
     "end_time": "2025-05-14T08:25:04.564544",
     "exception": false,
     "start_time": "2025-05-14T08:25:04.400438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "def build_mobilenet3(img_size=(224,224), weights=\"../input/cassava-leaf-disease-1st-place-models/cropnet_mobilenetv3/cropnet\"):\n",
    "    classifier = hub.KerasLayer(weights)\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=img_size + (3,)),\n",
    "    hub.KerasLayer(classifier, trainable=False)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014082,
     "end_time": "2025-05-14T08:25:04.597287",
     "exception": false,
     "start_time": "2025-05-14T08:25:04.583205",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Keras Inference with TTA\n",
    "\n",
    "For the included EfficientNets we used simple test time augmentations (Flip, Rotate, Transpose). To do this, we cropped 4 overlapping patches of size 512x512 from the .jpg images and applied 2 augmentations to each patch. We retain two additional center-cropped patches of the image to which no augmentations were applied. To get an overall prediction, we took the average of all these image tiles.\n",
    "\n",
    "For the CropNet, we just center-cropped and resized the image. In addition, we distributed the unknown class evenly over the 5 leaf diseases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T08:25:04.650345Z",
     "iopub.status.busy": "2025-05-14T08:25:04.635729Z",
     "iopub.status.idle": "2025-05-14T08:25:21.760599Z",
     "shell.execute_reply": "2025-05-14T08:25:21.760130Z"
    },
    "papermill": {
     "duration": 17.147753,
     "end_time": "2025-05-14T08:25:21.760731",
     "exception": false,
     "start_time": "2025-05-14T08:25:04.612978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.23s/it]\n"
     ]
    }
   ],
   "source": [
    "def image_augmentations(image):\n",
    "    p_spatial = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n",
    "    p_rotate = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n",
    "    \n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    \n",
    "    if p_spatial > 0.75:\n",
    "        image = tf.image.transpose(image)\n",
    "        \n",
    "    if p_rotate > 0.75:\n",
    "        image = tf.image.rot90(image, k = 3)\n",
    "    elif p_rotate > 0.5:\n",
    "        image = tf.image.rot90(image, k = 2)\n",
    "    elif p_rotate > 0.25:\n",
    "        image = tf.image.rot90(image, k = 1)\n",
    "\n",
    "    image = tf.image.resize(image, size = IMAGE_SIZE)\n",
    "    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
    "    \n",
    "    return image\n",
    "\n",
    "def read_preprocess_file(img_path, normalize=False):\n",
    "    image = Image.open(img_path)\n",
    "    if normalize:\n",
    "        img_scaled = np.array(image)/ 255.0\n",
    "    else:\n",
    "        img_scaled = np.array(image)\n",
    "    img_scaled = img_scaled.astype(np.float32)\n",
    "    return (image.size[0], image.size[1]), img_scaled\n",
    "\n",
    "def create_image_tiles(origin_dim, processed_img):\n",
    "    crop_size = 512\n",
    "    img_list = []\n",
    "    # Cut image into 4 overlapping patches\n",
    "    for x in [0, origin_dim[1] - crop_size]:\n",
    "        for y in [0, origin_dim[0] - crop_size]:\n",
    "            img_list.append(processed_img[x:x+crop_size , y:y+crop_size,:])\n",
    "    # Keep one additional center cropped image \n",
    "    img_list.append(cv2.resize(processed_img[:, 100:700 ,:], dsize=(crop_size, crop_size)))\n",
    "    return np.array(img_list)\n",
    "\n",
    "def augment_tiles_light(tiles, ttas=2):\n",
    "  # Copy central croped image to have same ratio to augmented images\n",
    "  holdout = np.broadcast_to(tiles[-1,:,:,:],(ttas,) + tiles.shape[1:])\n",
    "  augmented_batch = tf.map_fn(lambda x: image_augmentations(x), tf.concat(\n",
    "      [tiles[:-1,:,:,:] for _ in range(ttas)], axis=0))\n",
    "  return tf.concat([augmented_batch, holdout], axis=0)\n",
    "\n",
    "def cut_crop_image(processed_img):\n",
    "    image = tf.image.central_crop(processed_img, 0.8)\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    return np.expand_dims(image, 0)\n",
    "\n",
    "# CropNet class 6 (unknown) is distributed evenly over all 5 classes to match problem setting\n",
    "def distribute_unknown(propabilities):\n",
    "    return propabilities[:,:-1] + np.expand_dims(propabilities[:,-1]/5, 1)\n",
    "\n",
    "def multi_predict_tfhublayer(img_path, modelinstance):\n",
    "    img = cut_crop_image(read_preprocess_file(img_path, True)[1])\n",
    "    yhat = modelinstance.predict(img)\n",
    "    return np.mean(distribute_unknown(yhat), axis=0)\n",
    "\n",
    "def multi_predict_keras(img_path, modelinstance, *args):\n",
    "    augmented_batch = augment_tiles_light(create_image_tiles(\n",
    "        *read_preprocess_file(img_path)))\n",
    "    Yhat = modelinstance.predict(augmented_batch)\n",
    "    return np.mean(Yhat, axis=0)\n",
    "\n",
    "def predict_and_vote(image_list, modelinstances, onlykeras):\n",
    "    predictions = [] \n",
    "    with tqdm(total=len(image_list)) as process_bar:       \n",
    "      for img_path in image_list:\n",
    "        process_bar.update(1)  \n",
    "        Yhats = np.vstack([func(img_path, modelinstance) for func, modelinstance in modelinstances])\n",
    "        if onlykeras:\n",
    "            predictions.append(np.argmax(np.sum(Yhats, axis=0)))\n",
    "        else:\n",
    "            predictions.append(Yhats)    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "inference_models = []\n",
    "\n",
    "if \"mobilenet\" in used_models_keras:\n",
    "    model_mobilenet = build_mobilenet3(weights=used_models_keras[\"mobilenet\"])\n",
    "    inference_models.append((multi_predict_tfhublayer, model_mobilenet))\n",
    "    \n",
    "if \"efficientnetb4\" in used_models_keras:\n",
    "    model_efficientnetb4 =  keras.models.load_model(used_models_keras[\"efficientnetb4\"], compile=False)\n",
    "    inference_models.append((multi_predict_keras, model_efficientnetb4))\n",
    "    \n",
    "if \"efficientnetb5\" in used_models_keras:\n",
    "    model_efficientnetb5 =  keras.models.load_model(used_models_keras[\"efficientnetb5\"])\n",
    "    inference_models.append((multi_predict_keras, model_efficientnetb5))\n",
    "\n",
    "submission_df[\"label\"] = predict_and_vote([image_path+id for id in submission_df[\"image_id\"].values], inference_models, onlykeras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T08:25:21.792195Z",
     "iopub.status.busy": "2025-05-14T08:25:21.791414Z",
     "iopub.status.idle": "2025-05-14T08:25:22.036087Z",
     "shell.execute_reply": "2025-05-14T08:25:22.036501Z"
    },
    "papermill": {
     "duration": 0.262136,
     "end_time": "2025-05-14T08:25:22.036657",
     "exception": false,
     "start_time": "2025-05-14T08:25:21.774521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "746"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "try:\n",
    "    del inference_models[:]\n",
    "except:\n",
    "    pass\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012952,
     "end_time": "2025-05-14T08:25:22.062897",
     "exception": false,
     "start_time": "2025-05-14T08:25:22.049945",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Final Ensembling\n",
    "\n",
    "Our winning submission just included CropNet, EfficientNet B4, ResNext50 and ViT and a mean approach. We took the mean of the class weights from the ResNext and ViT model and combined this combination with the MobileNet and the EfficientnetB4 in the second stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T08:25:22.126153Z",
     "iopub.status.busy": "2025-05-14T08:25:22.119625Z",
     "iopub.status.idle": "2025-05-14T08:25:22.127949Z",
     "shell.execute_reply": "2025-05-14T08:25:22.128420Z",
     "shell.execute_reply.started": "2025-05-14T08:20:59.632511Z"
    },
    "papermill": {
     "duration": 0.052735,
     "end_time": "2025-05-14T08:25:22.128586",
     "exception": false,
     "start_time": "2025-05-14T08:25:22.075851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if len(list(used_models_keras.keys())) <= 1:\n",
    "    submission_df.loc[:,list(used_models_keras)[0]] = submission_df[\"label\"].explode()\n",
    "else:\n",
    "    tmp = (submission_df['label'].transform([lambda x:x[0], lambda x:x[1]]).set_axis(list(used_models_keras.keys()), axis=1, inplace=False))\n",
    "    submission_df = submission_df.merge(tmp, right_index=True, left_index=True)\n",
    "    \n",
    "submission_df[\"label\"] = 0\n",
    "\n",
    "if \"resnext\" in used_models_pytorch:\n",
    "    submission_df = submission_df.merge(predictions_resnext, on=\"image_id\")\n",
    "\n",
    "if \"HERB\" in used_models_pytorch:\n",
    "    submission_df = submission_df.merge(predictions_herb, on=\"image_id\")\n",
    "    \n",
    "if \"efficientnetb3\" in used_models_pytorch:\n",
    "    submission_df = submission_df.merge(predictions_cutmix, on=\"image_id\")\n",
    "    \n",
    "if \"vit2020\" in used_models_pytorch:\n",
    "    submission_df = submission_df.merge(predictions_vit, on=\"image_id\")\n",
    "    \n",
    "if \"vit2019\" in used_models_pytorch:\n",
    "    submission_df = submission_df.merge(predictions_vit2019, on=\"image_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T08:25:22.160717Z",
     "iopub.status.busy": "2025-05-14T08:25:22.160179Z",
     "iopub.status.idle": "2025-05-14T08:25:22.167115Z",
     "shell.execute_reply": "2025-05-14T08:25:22.166743Z",
     "shell.execute_reply.started": "2025-05-14T08:21:05.378164Z"
    },
    "papermill": {
     "duration": 0.025146,
     "end_time": "2025-05-14T08:25:22.167225",
     "exception": false,
     "start_time": "2025-05-14T08:25:22.142079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if stacked_mean:\n",
    "    submission_df[\"stage_1\"] = submission_df.apply(lambda row: [np.mean(e) for e in zip(row[\"vit2020\"], row[\"resnext\"])], axis=1)\n",
    "    submission_df[\"label\"] = submission_df.apply(lambda row: np.argmax(\n",
    "        [np.sum(e) for e in zip(row[\"mobilenet\"],row[\"stage_1\"], row[\"efficientnetb4\"], row['herb'])]), axis=1)        \n",
    "else:\n",
    "    submission_df[\"label\"] = submission_df.apply(lambda row: np.argmax(\n",
    "        [np.sum(e) for e in zip(*[row[m] for m in list(used_models_pytorch.keys())+list(used_models_keras.keys())])]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T08:25:22.202633Z",
     "iopub.status.busy": "2025-05-14T08:25:22.201961Z",
     "iopub.status.idle": "2025-05-14T08:25:22.210458Z",
     "shell.execute_reply": "2025-05-14T08:25:22.210057Z",
     "shell.execute_reply.started": "2025-05-14T08:20:44.428427Z"
    },
    "papermill": {
     "duration": 0.03025,
     "end_time": "2025-05-14T08:25:22.210574",
     "exception": false,
     "start_time": "2025-05-14T08:25:22.180324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>mobilenet</th>\n",
       "      <th>efficientnetb4</th>\n",
       "      <th>resnext</th>\n",
       "      <th>herb</th>\n",
       "      <th>vit2020</th>\n",
       "      <th>stage_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2216849948.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0039915927, 0.0037076094, 0.8709406, 0.0065...</td>\n",
       "      <td>[0.099482685, 0.1155221, 0.29643553, 0.0911106...</td>\n",
       "      <td>[0.030495381, 0.008266999, 0.5979506, 0.034923...</td>\n",
       "      <td>[0.08951104, 0.10328697, 0.19316715, 0.1098114...</td>\n",
       "      <td>[0.0029346435, 0.011939386, 0.62905705, 0.0144...</td>\n",
       "      <td>[0.016715012, 0.010103192, 0.6135038, 0.024666...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  label                                          mobilenet  \\\n",
       "0  2216849948.jpg      2  [0.0039915927, 0.0037076094, 0.8709406, 0.0065...   \n",
       "\n",
       "                                      efficientnetb4  \\\n",
       "0  [0.099482685, 0.1155221, 0.29643553, 0.0911106...   \n",
       "\n",
       "                                             resnext  \\\n",
       "0  [0.030495381, 0.008266999, 0.5979506, 0.034923...   \n",
       "\n",
       "                                                herb  \\\n",
       "0  [0.08951104, 0.10328697, 0.19316715, 0.1098114...   \n",
       "\n",
       "                                             vit2020  \\\n",
       "0  [0.0029346435, 0.011939386, 0.62905705, 0.0144...   \n",
       "\n",
       "                                             stage_1  \n",
       "0  [0.016715012, 0.010103192, 0.6135038, 0.024666...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T08:25:22.242158Z",
     "iopub.status.busy": "2025-05-14T08:25:22.241652Z",
     "iopub.status.idle": "2025-05-14T08:25:23.466137Z",
     "shell.execute_reply": "2025-05-14T08:25:23.465601Z",
     "shell.execute_reply.started": "2025-05-14T08:20:38.868392Z"
    },
    "papermill": {
     "duration": 1.242235,
     "end_time": "2025-05-14T08:25:23.466258",
     "exception": false,
     "start_time": "2025-05-14T08:25:22.224023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_id,label\r\n",
      "2216849948.jpg,2\r\n"
     ]
    }
   ],
   "source": [
    "submission_df[[\"image_id\",\"label\"]].to_csv(\"submission.csv\", index=False)\n",
    "!head submission.csv"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 1718836,
     "isSourceIdPinned": false,
     "sourceId": 13836,
     "sourceType": "competition"
    },
    {
     "datasetId": 1171087,
     "sourceId": 1971154,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7331722,
     "sourceId": 11681709,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7353604,
     "sourceId": 11715121,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30061,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 125.667358,
   "end_time": "2025-05-14T08:25:27.141322",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-14T08:23:21.473964",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
