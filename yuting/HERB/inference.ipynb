{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ad139d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CVML_4\\Desktop\\yuting - school\\VRDL\\final\\FGVC-HERBS-master\\timm\\__init__.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CVML_4\\anaconda3\\envs\\yuting_pytorch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from typing import Union\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "# sys.path.append('../FGVC-HERBS-master')\n",
    "import timm\n",
    "print(timm.__file__)\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f450849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ada8591",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transforms = transforms.Compose([\n",
    "        transforms.Resize((510, 510), Image.BILINEAR),\n",
    "        transforms.CenterCrop((384, 384)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "])\n",
    "\n",
    "\n",
    "class CassavaDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        img_path = os.path.join(self.img_dir, row[0])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = int(row[1])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return idx, image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1012dec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNCombiner(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 total_num_selects: int,\n",
    "                 num_classes: int, \n",
    "                 inputs: Union[dict, None] = None, \n",
    "                 proj_size: Union[int, None] = None,\n",
    "                 fpn_size: Union[int, None] = None):\n",
    "        \"\"\"\n",
    "        If building backbone without FPN, set fpn_size to None and MUST give \n",
    "        'inputs' and 'proj_size', the reason of these setting is to constrain the \n",
    "        dimension of graph convolutional network input.\n",
    "        \"\"\"\n",
    "        super(GCNCombiner, self).__init__()\n",
    "\n",
    "        assert inputs is not None or fpn_size is not None, \\\n",
    "            \"To build GCN combiner, you must give one features dimension.\"\n",
    "\n",
    "        ### auto-proj\n",
    "        self.fpn_size = fpn_size\n",
    "        if fpn_size is None:\n",
    "            for name in inputs:\n",
    "                if len(name) == 4:\n",
    "                    in_size = inputs[name].size(1)\n",
    "                elif len(name) == 3:\n",
    "                    in_size = inputs[name].size(2)\n",
    "                else:\n",
    "                    raise ValueError(\"The size of output dimension of previous must be 3 or 4.\")\n",
    "                m = nn.Sequential(\n",
    "                    nn.Linear(in_size, proj_size),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(proj_size, proj_size)\n",
    "                )\n",
    "                self.add_module(\"proj_\"+name, m)\n",
    "            self.proj_size = proj_size\n",
    "        else:\n",
    "            self.proj_size = fpn_size\n",
    "\n",
    "        ### build one layer structure (with adaptive module)\n",
    "        num_joints = total_num_selects // 64\n",
    "\n",
    "        self.param_pool0 = nn.Linear(total_num_selects, num_joints)\n",
    "        \n",
    "        A = torch.eye(num_joints) / 100 + 1 / 100\n",
    "        self.adj1 = nn.Parameter(copy.deepcopy(A))\n",
    "        self.conv1 = nn.Conv1d(self.proj_size, self.proj_size, 1)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(self.proj_size)\n",
    "        \n",
    "        self.conv_q1 = nn.Conv1d(self.proj_size, self.proj_size//4, 1)\n",
    "        self.conv_k1 = nn.Conv1d(self.proj_size, self.proj_size//4, 1)\n",
    "        self.alpha1 = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        ### merge information\n",
    "        self.param_pool1 = nn.Linear(num_joints, 1)\n",
    "        \n",
    "        #### class predict\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.classifier = nn.Linear(self.proj_size, num_classes)\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        hs = []\n",
    "        names = []\n",
    "        for name in x:\n",
    "            if \"FPN1_\" in name:\n",
    "                continue\n",
    "            if self.fpn_size is None:\n",
    "                _tmp = getattr(self, \"proj_\"+name)(x[name])\n",
    "            else:\n",
    "                _tmp = x[name]\n",
    "            hs.append(_tmp)\n",
    "            names.append([name, _tmp.size()])\n",
    "\n",
    "        hs = torch.cat(hs, dim=1).transpose(1, 2).contiguous() # B, S', C --> B, C, S\n",
    "        hs = self.param_pool0(hs)\n",
    "        ### adaptive adjacency\n",
    "        q1 = self.conv_q1(hs).mean(1)\n",
    "        k1 = self.conv_k1(hs).mean(1)\n",
    "        A1 = self.tanh(q1.unsqueeze(-1) - k1.unsqueeze(1))\n",
    "        A1 = self.adj1 + A1 * self.alpha1\n",
    "        ### graph convolution\n",
    "        hs = self.conv1(hs)\n",
    "        hs = torch.matmul(hs, A1)\n",
    "        hs = self.batch_norm1(hs)\n",
    "        ### predict\n",
    "        hs = self.param_pool1(hs)\n",
    "        hs = self.dropout(hs)\n",
    "        hs = hs.flatten(1)\n",
    "        hs = self.classifier(hs)\n",
    "\n",
    "        return hs\n",
    "    \n",
    "class WeaklySelector(nn.Module):\n",
    "\n",
    "    def __init__(self, inputs: dict, num_classes: int, num_select: dict, fpn_size: Union[int, None] = None):\n",
    "        \"\"\"\n",
    "        inputs: dictionary contain torch.Tensors, which comes from backbone\n",
    "                [Tensor1(hidden feature1), Tensor2(hidden feature2)...]\n",
    "                Please note that if len(features.size) equal to 3, the order of dimension must be [B,S,C],\n",
    "                S mean the spatial domain, and if len(features.size) equal to 4, the order must be [B,C,H,W]\n",
    "        \"\"\"\n",
    "        super(WeaklySelector, self).__init__()\n",
    "\n",
    "        self.num_select = num_select\n",
    "\n",
    "        self.fpn_size = fpn_size\n",
    "        ### build classifier\n",
    "        if self.fpn_size is None:\n",
    "            self.num_classes = num_classes\n",
    "            for name in inputs:\n",
    "                fs_size = inputs[name].size()\n",
    "                if len(fs_size) == 3:\n",
    "                    in_size = fs_size[2]\n",
    "                elif len(fs_size) == 4:\n",
    "                    in_size = fs_size[1]\n",
    "                m = nn.Linear(in_size, num_classes)\n",
    "                self.add_module(\"classifier_l_\"+name, m)\n",
    "\n",
    "        self.thresholds = {}\n",
    "        for name in inputs:\n",
    "            self.thresholds[name] = []\n",
    "\n",
    "    # def select(self, logits, l_name):\n",
    "    #     \"\"\"\n",
    "    #     logits: [B, S, num_classes]\n",
    "    #     \"\"\"\n",
    "    #     probs = torch.softmax(logits, dim=-1)\n",
    "    #     scores, _ = torch.max(probs, dim=-1)\n",
    "    #     _, ids = torch.sort(scores, -1, descending=True)\n",
    "    #     sn = self.num_select[l_name]\n",
    "    #     s_ids = ids[:, :sn]\n",
    "    #     not_s_ids = ids[:, sn:]\n",
    "    #     return s_ids.unsqueeze(-1), not_s_ids.unsqueeze(-1)\n",
    "\n",
    "    def forward(self, x, logits=None):\n",
    "        \"\"\"\n",
    "        x : \n",
    "            dictionary contain the features maps which \n",
    "            come from your choosen layers.\n",
    "            size must be [B, HxW, C] ([B, S, C]) or [B, C, H, W].\n",
    "            [B,C,H,W] will be transpose to [B, HxW, C] automatically.\n",
    "        \"\"\"\n",
    "        if self.fpn_size is None:\n",
    "            logits = {}\n",
    "        selections = {}\n",
    "        for name in x:\n",
    "            if \"FPN1_\" in name:\n",
    "                continue\n",
    "            if len(x[name].size()) == 4:\n",
    "                B, C, H, W = x[name].size()\n",
    "                x[name] = x[name].view(B, C, H*W).permute(0, 2, 1).contiguous()\n",
    "            C = x[name].size(-1)\n",
    "            if self.fpn_size is None:\n",
    "                logits[name] = getattr(self, \"classifier_l_\"+name)(x[name])\n",
    "            \n",
    "            probs = torch.softmax(logits[name], dim=-1)\n",
    "            sum_probs = torch.softmax(logits[name].mean(1), dim=-1)\n",
    "            selections[name] = []\n",
    "            preds_1 = []\n",
    "            preds_0 = []\n",
    "            num_select = self.num_select[name]\n",
    "            for bi in range(logits[name].size(0)):\n",
    "                _, max_ids = torch.max(sum_probs[bi], dim=-1)\n",
    "                confs, ranks = torch.sort(probs[bi, :, max_ids], descending=True)\n",
    "                sf = x[name][bi][ranks[:num_select]]\n",
    "                nf = x[name][bi][ranks[num_select:]]  # calculate\n",
    "                selections[name].append(sf) # [num_selected, C]\n",
    "                preds_1.append(logits[name][bi][ranks[:num_select]])\n",
    "                preds_0.append(logits[name][bi][ranks[num_select:]])\n",
    "\n",
    "                if bi >= len(self.thresholds[name]):\n",
    "                    self.thresholds[name].append(confs[num_select]) # for initialize\n",
    "                else:\n",
    "                    self.thresholds[name][bi] = confs[num_select]\n",
    "            \n",
    "            selections[name] = torch.stack(selections[name])\n",
    "            preds_1 = torch.stack(preds_1)\n",
    "            preds_0 = torch.stack(preds_0)\n",
    "\n",
    "            logits[\"select_\"+name] = preds_1\n",
    "            logits[\"drop_\"+name] = preds_0\n",
    "\n",
    "        return selections\n",
    "    \n",
    "class FPN(nn.Module):\n",
    "\n",
    "    def __init__(self, inputs: dict, fpn_size: int, proj_type: str, upsample_type: str):\n",
    "        \"\"\"\n",
    "        inputs : dictionary contains torch.Tensor\n",
    "                 which comes from backbone output\n",
    "        fpn_size: integer, fpn \n",
    "        proj_type: \n",
    "            in [\"Conv\", \"Linear\"]\n",
    "        upsample_type:\n",
    "            in [\"Bilinear\", \"Conv\", \"Fc\"]\n",
    "            for convolution neural network (e.g. ResNet, EfficientNet), recommand 'Bilinear'. \n",
    "            for Vit, \"Fc\". and Swin-T, \"Conv\"\n",
    "        \"\"\"\n",
    "        super(FPN, self).__init__()\n",
    "        assert proj_type in [\"Conv\", \"Linear\"], \\\n",
    "            \"FPN projection type {} were not support yet, please choose type 'Conv' or 'Linear'\".format(proj_type)\n",
    "        assert upsample_type in [\"Bilinear\", \"Conv\"], \\\n",
    "            \"FPN upsample type {} were not support yet, please choose type 'Bilinear' or 'Conv'\".format(proj_type)\n",
    "\n",
    "        self.fpn_size = fpn_size\n",
    "        self.upsample_type = upsample_type\n",
    "        inp_names = [name for name in inputs]\n",
    "        for i, node_name in enumerate(inputs):\n",
    "            ### projection module\n",
    "            if proj_type == \"Conv\":\n",
    "                m = nn.Sequential(\n",
    "                    nn.Conv2d(inputs[node_name].size(1), inputs[node_name].size(1), 1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv2d(inputs[node_name].size(1), fpn_size, 1)\n",
    "                )\n",
    "            elif proj_type == \"Linear\":\n",
    "                m = nn.Sequential(\n",
    "                    nn.Linear(inputs[node_name].size(-1), inputs[node_name].size(-1)),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(inputs[node_name].size(-1), fpn_size),\n",
    "                )\n",
    "            self.add_module(\"Proj_\"+node_name, m)\n",
    "\n",
    "            ### upsample module\n",
    "            if upsample_type == \"Conv\" and i != 0:\n",
    "                assert len(inputs[node_name].size()) == 3 # B, S, C\n",
    "                in_dim = inputs[node_name].size(1)\n",
    "                out_dim = inputs[inp_names[i-1]].size(1)\n",
    "                # if in_dim != out_dim:\n",
    "                m = nn.Conv1d(in_dim, out_dim, 1) # for spatial domain\n",
    "                # else:\n",
    "                #     m = nn.Identity()\n",
    "                self.add_module(\"Up_\"+node_name, m)\n",
    "\n",
    "        if upsample_type == \"Bilinear\":\n",
    "            self.upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "\n",
    "    def upsample_add(self, x0: torch.Tensor, x1: torch.Tensor, x1_name: str):\n",
    "        \"\"\"\n",
    "        return Upsample(x1) + x1\n",
    "        \"\"\"\n",
    "        if self.upsample_type == \"Bilinear\":\n",
    "            if x1.size(-1) != x0.size(-1):\n",
    "                x1 = self.upsample(x1)\n",
    "        else:\n",
    "            x1 = getattr(self, \"Up_\"+x1_name)(x1)\n",
    "        return x1 + x0\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x : dictionary\n",
    "            {\n",
    "                \"node_name1\": feature1,\n",
    "                \"node_name2\": feature2, ...\n",
    "            }\n",
    "        \"\"\"\n",
    "        ### project to same dimension\n",
    "        hs = []\n",
    "        for i, name in enumerate(x):\n",
    "            if \"FPN1_\" in name:\n",
    "                continue\n",
    "            x[name] = getattr(self, \"Proj_\"+name)(x[name])\n",
    "            hs.append(name)\n",
    "        \n",
    "        x[\"FPN1_\" + \"layer4\"] = x[\"layer4\"]\n",
    "\n",
    "        for i in range(len(hs)-1, 0, -1):\n",
    "            x1_name = hs[i]\n",
    "            x0_name = hs[i-1]\n",
    "            x[x0_name] = self.upsample_add(x[x0_name], \n",
    "                                           x[x1_name], \n",
    "                                           x1_name)\n",
    "            x[\"FPN1_\" + x0_name] = x[x0_name]\n",
    "\n",
    "        return x\n",
    "\n",
    "class FPN_UP(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 inputs: dict, \n",
    "                 fpn_size: int):\n",
    "        super(FPN_UP, self).__init__()\n",
    "\n",
    "        inp_names = [name for name in inputs]\n",
    "\n",
    "        for i, node_name in enumerate(inputs):\n",
    "            ### projection module\n",
    "            m = nn.Sequential(\n",
    "                nn.Linear(fpn_size, fpn_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fpn_size, fpn_size),\n",
    "            )\n",
    "            self.add_module(\"Proj_\"+node_name, m)\n",
    "\n",
    "            ### upsample module\n",
    "            if i != (len(inputs) - 1):\n",
    "                assert len(inputs[node_name].size()) == 3 # B, S, C\n",
    "                in_dim = inputs[node_name].size(1)\n",
    "                out_dim = inputs[inp_names[i+1]].size(1)\n",
    "                m = nn.Conv1d(in_dim, out_dim, 1) # for spatial domain\n",
    "                self.add_module(\"Down_\"+node_name, m)\n",
    "                \"\"\"\n",
    "                Down_layer1 2304 576\n",
    "                Down_layer2 576 144\n",
    "                Down_layer3 144 144\n",
    "                \"\"\"\n",
    "\n",
    "    def downsample_add(self, x0: torch.Tensor, x1: torch.Tensor, x0_name: str):\n",
    "        \"\"\"\n",
    "        return Upsample(x1) + x1\n",
    "        \"\"\"\n",
    "        # print(\"[downsample_add] Down_\" + x0_name)\n",
    "        x0 = getattr(self, \"Down_\" + x0_name)(x0)\n",
    "        return x1 + x0\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x : dictionary\n",
    "            {\n",
    "                \"node_name1\": feature1,\n",
    "                \"node_name2\": feature2, ...\n",
    "            }\n",
    "        \"\"\"\n",
    "        ### project to same dimension\n",
    "        hs = []\n",
    "        for i, name in enumerate(x):\n",
    "            if \"FPN1_\" in name:\n",
    "                continue\n",
    "            x[name] = getattr(self, \"Proj_\"+name)(x[name])\n",
    "            hs.append(name)\n",
    "\n",
    "        # print(hs)\n",
    "        for i in range(0, len(hs) - 1):\n",
    "            x0_name = hs[i]\n",
    "            x1_name = hs[i+1]\n",
    "            # print(x0_name, x1_name)\n",
    "            # print(x[x0_name].size(), x[x1_name].size())\n",
    "            x[x1_name] = self.downsample_add(x[x0_name], \n",
    "                                             x[x1_name], \n",
    "                                             x0_name)\n",
    "        return x\n",
    "\n",
    "class PluginMoodel(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 backbone: torch.nn.Module,\n",
    "                 return_nodes: Union[dict, None],\n",
    "                 img_size: int,\n",
    "                 use_fpn: bool,\n",
    "                 fpn_size: Union[int, None],\n",
    "                 proj_type: str,\n",
    "                 upsample_type: str,\n",
    "                 use_selection: bool,\n",
    "                 num_classes: int,\n",
    "                 num_selects: dict, \n",
    "                 use_combiner: bool,\n",
    "                 comb_proj_size: Union[int, None]\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        * backbone: \n",
    "            torch.nn.Module class (recommand pretrained on ImageNet or IG-3.5B-17k(provided by FAIR))\n",
    "        * return_nodes:\n",
    "            e.g.\n",
    "            return_nodes = {\n",
    "                # node_name: user-specified key for output dict\n",
    "                'layer1.2.relu_2': 'layer1',\n",
    "                'layer2.3.relu_2': 'layer2',\n",
    "                'layer3.5.relu_2': 'layer3',\n",
    "                'layer4.2.relu_2': 'layer4',\n",
    "            } # you can see the example on https://pytorch.org/vision/main/feature_extraction.html\n",
    "            !!! if using 'Swin-Transformer', please set return_nodes to None\n",
    "            !!! and please set use_fpn to True\n",
    "        * feat_sizes: \n",
    "            tuple or list contain features map size of each layers. \n",
    "            ((C, H, W)). e.g. ((1024, 14, 14), (2048, 7, 7))\n",
    "        * use_fpn: \n",
    "            boolean, use features pyramid network or not\n",
    "        * fpn_size: \n",
    "            integer, features pyramid network projection dimension\n",
    "        * num_selects:\n",
    "            num_selects = {\n",
    "                # match user-specified in return_nodes\n",
    "                \"layer1\": 2048,\n",
    "                \"layer2\": 512,\n",
    "                \"layer3\": 128,\n",
    "                \"layer4\": 32,\n",
    "            }\n",
    "        Note: after selector module (WeaklySelector) , the feature map's size is [B, S', C] which \n",
    "        contained by 'logits' or 'selections' dictionary (S' is selection number, different layer \n",
    "        could be different).\n",
    "        \"\"\"\n",
    "        super(PluginMoodel, self).__init__()\n",
    "        \n",
    "        ### = = = = = Backbone = = = = =\n",
    "        self.return_nodes = return_nodes\n",
    "        if return_nodes is not None:\n",
    "            self.backbone = create_feature_extractor(backbone, return_nodes=return_nodes)\n",
    "        else:\n",
    "            self.backbone = backbone\n",
    "        \n",
    "        ### get hidden feartues size\n",
    "        rand_in = torch.randn(1, 3, img_size, img_size)\n",
    "        outs = self.backbone(rand_in)\n",
    "        # print('outs')\n",
    "        # print(outs)\n",
    "\n",
    "        ### just original backbone\n",
    "        if not use_fpn and (not use_selection and not use_combiner):\n",
    "            for name in outs:\n",
    "                fs_size = outs[name].size()\n",
    "                if len(fs_size) == 3:\n",
    "                    out_size = fs_size.size(-1)\n",
    "                elif len(fs_size) == 4:\n",
    "                    out_size = fs_size.size(1)\n",
    "                else:\n",
    "                    raise ValueError(\"The size of output dimension of previous must be 3 or 4.\")\n",
    "            self.classifier = nn.Linear(out_size, num_classes)\n",
    "\n",
    "        ### = = = = = FPN = = = = =\n",
    "        self.use_fpn = use_fpn\n",
    "        if self.use_fpn:\n",
    "            self.fpn_down = FPN(outs, fpn_size, proj_type, upsample_type)\n",
    "            self.build_fpn_classifier_down(outs, fpn_size, num_classes)\n",
    "            self.fpn_up = FPN_UP(outs, fpn_size)\n",
    "            self.build_fpn_classifier_up(outs, fpn_size, num_classes)\n",
    "\n",
    "        self.fpn_size = fpn_size\n",
    "\n",
    "        ### = = = = = Selector = = = = =\n",
    "        self.use_selection = use_selection\n",
    "        if self.use_selection:\n",
    "            w_fpn_size = self.fpn_size if self.use_fpn else None # if not using fpn, build classifier in weakly selector\n",
    "            self.selector = WeaklySelector(outs, num_classes, num_selects, w_fpn_size)\n",
    "\n",
    "        ### = = = = = Combiner = = = = =\n",
    "        self.use_combiner = use_combiner\n",
    "        if self.use_combiner:\n",
    "            assert self.use_selection, \"Please use selection module before combiner\"\n",
    "            if self.use_fpn:\n",
    "                gcn_inputs, gcn_proj_size = None, None\n",
    "            else:\n",
    "                gcn_inputs, gcn_proj_size = outs, comb_proj_size # redundant, fix in future\n",
    "            total_num_selects = sum([num_selects[name] for name in num_selects]) # sum\n",
    "            self.combiner = GCNCombiner(total_num_selects, num_classes, gcn_inputs, gcn_proj_size, self.fpn_size)\n",
    "\n",
    "    def build_fpn_classifier_up(self, inputs: dict, fpn_size: int, num_classes: int):\n",
    "        \"\"\"\n",
    "        Teh results of our experiments show that linear classifier in this case may cause some problem.\n",
    "        \"\"\"\n",
    "        for name in inputs:\n",
    "            m = nn.Sequential(\n",
    "                    nn.Conv1d(fpn_size, fpn_size, 1),\n",
    "                    nn.BatchNorm1d(fpn_size),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv1d(fpn_size, num_classes, 1)\n",
    "                )\n",
    "            self.add_module(\"fpn_classifier_up_\"+name, m)\n",
    "\n",
    "    def build_fpn_classifier_down(self, inputs: dict, fpn_size: int, num_classes: int):\n",
    "        \"\"\"\n",
    "        Teh results of our experiments show that linear classifier in this case may cause some problem.\n",
    "        \"\"\"\n",
    "        for name in inputs:\n",
    "            m = nn.Sequential(\n",
    "                    nn.Conv1d(fpn_size, fpn_size, 1),\n",
    "                    nn.BatchNorm1d(fpn_size),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv1d(fpn_size, num_classes, 1)\n",
    "                )\n",
    "            self.add_module(\"fpn_classifier_down_\" + name, m)\n",
    "\n",
    "    def forward_backbone(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "    def fpn_predict_down(self, x: dict, logits: dict):\n",
    "        \"\"\"\n",
    "        x: [B, C, H, W] or [B, S, C]\n",
    "           [B, C, H, W] --> [B, H*W, C]\n",
    "        \"\"\"\n",
    "        for name in x:\n",
    "            if \"FPN1_\" not in name:\n",
    "                continue \n",
    "            ### predict on each features point\n",
    "            if len(x[name].size()) == 4:\n",
    "                B, C, H, W = x[name].size()\n",
    "                logit = x[name].view(B, C, H*W)\n",
    "            elif len(x[name].size()) == 3:\n",
    "                logit = x[name].transpose(1, 2).contiguous()\n",
    "            model_name = name.replace(\"FPN1_\", \"\")\n",
    "            logits[name] = getattr(self, \"fpn_classifier_down_\" + model_name)(logit)\n",
    "            logits[name] = logits[name].transpose(1, 2).contiguous() # transpose\n",
    "\n",
    "    def fpn_predict_up(self, x: dict, logits: dict):\n",
    "        \"\"\"\n",
    "        x: [B, C, H, W] or [B, S, C]\n",
    "           [B, C, H, W] --> [B, H*W, C]\n",
    "        \"\"\"\n",
    "        for name in x:\n",
    "            if \"FPN1_\" in name:\n",
    "                continue\n",
    "            ### predict on each features point\n",
    "            if len(x[name].size()) == 4:\n",
    "                B, C, H, W = x[name].size()\n",
    "                logit = x[name].view(B, C, H*W)\n",
    "            elif len(x[name].size()) == 3:\n",
    "                logit = x[name].transpose(1, 2).contiguous()\n",
    "            model_name = name.replace(\"FPN1_\", \"\")\n",
    "            logits[name] = getattr(self, \"fpn_classifier_up_\" + model_name)(logit)\n",
    "            logits[name] = logits[name].transpose(1, 2).contiguous() # transpose\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "\n",
    "        logits = {}\n",
    "\n",
    "        x = self.forward_backbone(x)\n",
    "\n",
    "        if self.use_fpn:\n",
    "            x = self.fpn_down(x)\n",
    "            # print([name for name in x])\n",
    "            self.fpn_predict_down(x, logits)\n",
    "            x = self.fpn_up(x)\n",
    "            self.fpn_predict_up(x, logits)\n",
    "\n",
    "        if self.use_selection:\n",
    "            selects = self.selector(x, logits)\n",
    "\n",
    "        if self.use_combiner:\n",
    "            comb_outs = self.combiner(selects)\n",
    "            logits['comb_outs'] = comb_outs\n",
    "            return logits\n",
    "        \n",
    "        if self.use_selection or self.fpn:\n",
    "            return logits\n",
    "\n",
    "        ### original backbone (only predict final selected layer)\n",
    "        for name in x:\n",
    "            hs = x[name]\n",
    "\n",
    "        if len(hs.size()) == 4:\n",
    "            hs = F.adaptive_avg_pool2d(hs, (1, 1))\n",
    "            hs = hs.flatten(1)\n",
    "        else:\n",
    "            hs = hs.mean(1)\n",
    "        out = self.classifier(hs)\n",
    "        logits['ori_out'] = logits\n",
    "\n",
    "        return logits\n",
    "\n",
    "@torch.no_grad()\n",
    "def sum_all_out(out, sum_type=\"softmax\"):\n",
    "    target_layer_names = \\\n",
    "    ['layer1', 'layer2', 'layer3', 'layer4',\n",
    "    'FPN1_layer1', 'FPN1_layer2', 'FPN1_layer3', 'FPN1_layer4', \n",
    "    'comb_outs']\n",
    "\n",
    "    sum_out = None\n",
    "    for name in target_layer_names:\n",
    "        if name != \"comb_outs\":\n",
    "            tmp_out = out[name].mean(1)\n",
    "        else:\n",
    "            tmp_out = out[name]\n",
    "        \n",
    "        if sum_type == \"softmax\":\n",
    "            tmp_out = torch.softmax(tmp_out, dim=-1)\n",
    "        if sum_out is None:\n",
    "            sum_out = tmp_out\n",
    "        else:\n",
    "            sum_out = sum_out + tmp_out # note that use '+=' would cause inplace error\n",
    "    return sum_out\n",
    "\n",
    "def build_model(pretrainewd_path: str,\n",
    "                img_size: int, \n",
    "                fpn_size: int, \n",
    "                num_classes: int,\n",
    "                num_selects: dict,\n",
    "                use_fpn: bool = True, \n",
    "                use_selection: bool = True,\n",
    "                use_combiner: bool = True, \n",
    "                comb_proj_size: int = None):\n",
    "    model = \\\n",
    "        PluginMoodel(backbone=timm.create_model('swin_large_patch4_window12_384_in22k'),\n",
    "                     return_nodes=None,\n",
    "                     img_size = img_size,\n",
    "                     use_fpn = use_fpn,\n",
    "                     fpn_size = fpn_size,\n",
    "                     proj_type = \"Linear\",\n",
    "                     upsample_type = \"Conv\",\n",
    "                     use_selection = use_selection,\n",
    "                     num_classes = num_classes,\n",
    "                     num_selects = num_selects, \n",
    "                     use_combiner = use_combiner,\n",
    "                     comb_proj_size = comb_proj_size)\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "949275eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./records/FGVC-HERBS/basline/backup/fold_1_best.pt', './records/FGVC-HERBS/basline/backup/fold_2_best.pt', './records/FGVC-HERBS/basline/backup/fold_3_best.pt', './records/FGVC-HERBS/basline/backup/fold_4_best.pt', './records/FGVC-HERBS/basline/backup/fold_5_best.pt']\n"
     ]
    }
   ],
   "source": [
    "used_models_pytorch = {\"HERB\": [f'./records/FGVC-HERBS/basline/backup/fold_{fold}_best.pt' for fold in [1,2,3,4,5]],}\n",
    "print(used_models_pytorch['HERB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0208ef4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../input/cassava-leaf-disease-classification/\"\n",
    "image_path = path+\"test_images/\"\n",
    "\n",
    "submission_df = pd.DataFrame(columns=[\"image_id\",\"label\"])\n",
    "submission_df[\"image_id\"] = os.listdir(image_path)\n",
    "submission_df[\"label\"] = 0\n",
    "\n",
    "submission_df[[\"image_id\",\"label\"]].to_csv(\"../input/cassava-leaf-disease-classification/sample_submission.csv\", index=False)\n",
    "\n",
    "def inference(model, states, test_loader, device):\n",
    "        model.to(device)\n",
    "\n",
    "        probabilities = []\n",
    "        for i, (_, images, _) in enumerate(test_loader):\n",
    "            images = images.to(device)\n",
    "            avg_preds = []\n",
    "            for state in states:\n",
    "                model.load_state_dict(state['model'])\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    outs = model(images)\n",
    "                    outs = sum_all_out(outs, sum_type=\"softmax\") # softmax\n",
    "                    # print(outs.shape)\n",
    "                avg_preds.append(outs.to('cpu').numpy())\n",
    "            avg_preds = np.mean(avg_preds, axis=0)\n",
    "            probabilities.append(avg_preds)\n",
    "        return np.concatenate(probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8548919",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CVML_4\\anaconda3\\envs\\yuting_pytorch\\Lib\\site-packages\\torch\\functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3610.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "C:\\Users\\CVML_4\\AppData\\Local\\Temp\\ipykernel_18048\\1464424370.py:23: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  img_path = os.path.join(self.img_dir, row[0])\n",
      "C:\\Users\\CVML_4\\AppData\\Local\\Temp\\ipykernel_18048\\1464424370.py:25: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  label = int(row[1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m model.eval()\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     outs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m     outs = sum_all_out(outs, sum_type=\u001b[33m\"\u001b[39m\u001b[33msoftmax\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;66;03m# softmax\u001b[39;00m\n\u001b[32m     29\u001b[39m avg_preds.append(outs.to(\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m).numpy())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CVML_4\\anaconda3\\envs\\yuting_pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CVML_4\\anaconda3\\envs\\yuting_pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 518\u001b[39m, in \u001b[36mPluginMoodel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    514\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor):\n\u001b[32m    516\u001b[39m     logits = {}\n\u001b[32m--> \u001b[39m\u001b[32m518\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward_backbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    520\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_fpn:\n\u001b[32m    521\u001b[39m         x = \u001b[38;5;28mself\u001b[39m.fpn_down(x)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 476\u001b[39m, in \u001b[36mPluginMoodel.forward_backbone\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward_backbone\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m476\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CVML_4\\anaconda3\\envs\\yuting_pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CVML_4\\anaconda3\\envs\\yuting_pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CVML_4\\Desktop\\yuting - school\\VRDL\\final\\FGVC-HERBS-master\\timm\\models\\swin_transformer.py:544\u001b[39m, in \u001b[36mSwinTransformer.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    543\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m544\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    545\u001b[39m     \u001b[38;5;66;03m# x = self.head(x)\u001b[39;00m\n\u001b[32m    546\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CVML_4\\Desktop\\yuting - school\\VRDL\\final\\FGVC-HERBS-master\\timm\\models\\swin_transformer.py:530\u001b[39m, in \u001b[36mSwinTransformer.forward_features\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    529\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m530\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpatch_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    531\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.absolute_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    532\u001b[39m         x = x + \u001b[38;5;28mself\u001b[39m.absolute_pos_embed\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CVML_4\\anaconda3\\envs\\yuting_pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CVML_4\\anaconda3\\envs\\yuting_pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CVML_4\\Desktop\\yuting - school\\VRDL\\final\\FGVC-HERBS-master\\timm\\models\\layers\\patch_embed.py:35\u001b[39m, in \u001b[36mPatchEmbed.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     32\u001b[39m B, C, H, W = x.shape\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# assert H == self.img_size[0] and W == self.img_size[1], \\\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m#     f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mproj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.flatten:\n\u001b[32m     37\u001b[39m     x = x.flatten(\u001b[32m2\u001b[39m).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# BCHW -> BNC\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CVML_4\\anaconda3\\envs\\yuting_pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CVML_4\\anaconda3\\envs\\yuting_pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CVML_4\\anaconda3\\envs\\yuting_pytorch\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:458\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m458\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CVML_4\\anaconda3\\envs\\yuting_pytorch\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:454\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m'\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    451\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(F.pad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode),\n\u001b[32m    452\u001b[39m                     weight, bias, \u001b[38;5;28mself\u001b[39m.stride,\n\u001b[32m    453\u001b[39m                     _pair(\u001b[32m0\u001b[39m), \u001b[38;5;28mself\u001b[39m.dilation, \u001b[38;5;28mself\u001b[39m.groups)\n\u001b[32m--> \u001b[39m\u001b[32m454\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "    pretrainewd_path='',\n",
    "    img_size=384,\n",
    "    fpn_size=1536,\n",
    "    num_classes=5,\n",
    "    num_selects={'layer1': 256, 'layer2': 128, 'layer3': 64,'layer4': 32}\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "states = [torch.load(f, weights_only=False, map_location=\"cuda:0\") for f in used_models_pytorch[\"HERB\"]]\n",
    "\n",
    "# 4. Predict on test data\n",
    "test_df = pd.read_csv('../input/cassava-leaf-disease-classification/sample_submission.csv')\n",
    "test_dataset = CassavaDataset('../input/cassava-leaf-disease-classification/sample_submission.csv', '../input/cassava-leaf-disease-classification/test_images', transform=val_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# myDataloader = DataLoader(myCassavaDataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "# Predict on test data\n",
    "predictions = []\n",
    "for _, images, _ in test_loader:\n",
    "    images = images.to(device)\n",
    "    avg_preds = []\n",
    "    for state in states:\n",
    "        model.load_state_dict(state['model'])\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outs = model(images)\n",
    "            outs = sum_all_out(outs, sum_type=\"softmax\") # softmax\n",
    "        avg_preds.append(outs.to('cpu').numpy())\n",
    "    avg_preds = np.mean(avg_preds, axis=0)\n",
    "    avg_preds = torch.tensor(avg_preds)  \n",
    "    _, preds = torch.max(avg_preds, 1)\n",
    "    predictions.extend(preds.cpu().numpy())\n",
    "    \n",
    "# Convert predictions to a NumPy array\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "# 5. Create submission file\n",
    "test_df['label'] = predictions\n",
    "test_df.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file created!\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yuting_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
