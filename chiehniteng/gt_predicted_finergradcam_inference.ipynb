{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1576b2a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:07:18.795111Z",
     "iopub.status.busy": "2025-05-19T07:07:18.794787Z",
     "iopub.status.idle": "2025-05-19T07:08:40.270136Z",
     "shell.execute_reply": "2025-05-19T07:08:40.269075Z"
    },
    "papermill": {
     "duration": 81.482762,
     "end_time": "2025-05-19T07:08:40.273001",
     "exception": false,
     "start_time": "2025-05-19T07:07:18.790239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/gradcam-wheel/ttach-0.0.3-py3-none-any.whl\r\n",
      "Installing collected packages: ttach\r\n",
      "Successfully installed ttach-0.0.3\r\n",
      "Processing /kaggle/input/gradcam-wheel/grad_cam-1.5.5-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from grad-cam==1.5.5) (1.26.4)\r\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from grad-cam==1.5.5) (10.3.0)\r\n",
      "Requirement already satisfied: torch>=1.7.1 in /opt/conda/lib/python3.10/site-packages (from grad-cam==1.5.5) (2.4.0)\r\n",
      "Requirement already satisfied: torchvision>=0.8.2 in /opt/conda/lib/python3.10/site-packages (from grad-cam==1.5.5) (0.19.0)\r\n",
      "Requirement already satisfied: ttach in /opt/conda/lib/python3.10/site-packages (from grad-cam==1.5.5) (0.0.3)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from grad-cam==1.5.5) (4.66.4)\r\n",
      "Requirement already satisfied: opencv-python in /opt/conda/lib/python3.10/site-packages (from grad-cam==1.5.5) (4.10.0.84)\r\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from grad-cam==1.5.5) (3.7.5)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from grad-cam==1.5.5) (1.2.2)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->grad-cam==1.5.5) (3.15.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->grad-cam==1.5.5) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->grad-cam==1.5.5) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->grad-cam==1.5.5) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->grad-cam==1.5.5) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->grad-cam==1.5.5) (2024.6.1)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->grad-cam==1.5.5) (1.2.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->grad-cam==1.5.5) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->grad-cam==1.5.5) (4.53.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->grad-cam==1.5.5) (1.4.5)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->grad-cam==1.5.5) (21.3)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->grad-cam==1.5.5) (3.1.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->grad-cam==1.5.5) (2.9.0.post0)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->grad-cam==1.5.5) (1.14.1)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->grad-cam==1.5.5) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->grad-cam==1.5.5) (3.5.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->grad-cam==1.5.5) (1.16.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.7.1->grad-cam==1.5.5) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.7.1->grad-cam==1.5.5) (1.3.0)\r\n",
      "Installing collected packages: grad-cam\r\n",
      "Successfully installed grad-cam-1.5.5\r\n"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/gradcam-wheel/ttach-0.0.3-py3-none-any.whl\n",
    "!pip install /kaggle/input/gradcam-wheel/grad_cam-1.5.5-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22afb5a5",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-19T07:08:40.281054Z",
     "iopub.status.busy": "2025-05-19T07:08:40.280746Z",
     "iopub.status.idle": "2025-05-19T07:08:46.034018Z",
     "shell.execute_reply": "2025-05-19T07:08:46.033139Z"
    },
    "papermill": {
     "duration": 5.759527,
     "end_time": "2025-05-19T07:08:46.036083",
     "exception": false,
     "start_time": "2025-05-19T07:08:40.276556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2054ebe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:08:46.044108Z",
     "iopub.status.busy": "2025-05-19T07:08:46.043728Z",
     "iopub.status.idle": "2025-05-19T07:09:20.569807Z",
     "shell.execute_reply": "2025-05-19T07:09:20.569046Z"
    },
    "papermill": {
     "duration": 34.532164,
     "end_time": "2025-05-19T07:09:20.571752",
     "exception": false,
     "start_time": "2025-05-19T07:08:46.039588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/albumentations/check_version.py:49: UserWarning: Error fetching version info <urlopen error [Errno -3] Temporary failure in name resolution>\n",
      "  data = fetch_version_info()\n"
     ]
    }
   ],
   "source": [
    "from albumentations import (Compose, Normalize, Resize, RandomResizedCrop, HorizontalFlip, VerticalFlip, ShiftScaleRotate, Transpose)\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from collections import OrderedDict\n",
    "import cv2\n",
    "import math\n",
    "from pytorch_grad_cam import GradCAMPlusPlus\n",
    "from torchvision.models import resnext50_32x4d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "888b2d4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:09:20.579426Z",
     "iopub.status.busy": "2025-05-19T07:09:20.578974Z",
     "iopub.status.idle": "2025-05-19T07:09:20.643048Z",
     "shell.execute_reply": "2025-05-19T07:09:20.642060Z"
    },
    "papermill": {
     "duration": 0.069824,
     "end_time": "2025-05-19T07:09:20.644877",
     "exception": false,
     "start_time": "2025-05-19T07:09:20.575053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c158a26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:09:20.653597Z",
     "iopub.status.busy": "2025-05-19T07:09:20.653284Z",
     "iopub.status.idle": "2025-05-19T07:09:20.657511Z",
     "shell.execute_reply": "2025-05-19T07:09:20.656705Z"
    },
    "papermill": {
     "duration": 0.010899,
     "end_time": "2025-05-19T07:09:20.659149",
     "exception": false,
     "start_time": "2025-05-19T07:09:20.648250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_PATH = '../input/cassava-leaf-disease-classification/train_images'\n",
    "TEST_PATH = '../input/cassava-leaf-disease-classification/test_images'\n",
    "OUTPUT_DIR = './'\n",
    "SUB_FILE = '../input/cassava-leaf-disease-classification/sample_submission.csv'\n",
    "FIVEFOLD = \"/kaggle/input/cassava-5-fold-csv/validation_data.csv\"\n",
    "\n",
    "STAGE_ONE_MODEL = \"/kaggle/input/finergradcam/pytorch/default/1/gan_v3_fold0_best_g_g.pth\"\n",
    "STAGE_TWO_MODEL = \"/kaggle/input/finergradcam/pytorch/default/1/finergradcam_fold0_best_d_d.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3012e891",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:09:20.667821Z",
     "iopub.status.busy": "2025-05-19T07:09:20.667601Z",
     "iopub.status.idle": "2025-05-19T07:09:20.690948Z",
     "shell.execute_reply": "2025-05-19T07:09:20.690399Z"
    },
    "papermill": {
     "duration": 0.029021,
     "end_time": "2025-05-19T07:09:20.692463",
     "exception": false,
     "start_time": "2025-05-19T07:09:20.663442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    print_freq=300\n",
    "    num_workers = 4\n",
    "    model_name = 'finergradcam'\n",
    "    size = 512\n",
    "    epochs = 8\n",
    "    factor = 0.2\n",
    "    patience = 5\n",
    "    eps = 1e-6\n",
    "    lr = 1e-4\n",
    "    min_lr = 1e-6\n",
    "    batch_size = 12\n",
    "    weight_decay = 1e-6\n",
    "    gradient_accumulation_steps = 1\n",
    "    max_grad_norm = 1000\n",
    "    seed = 42\n",
    "    target_size = 5\n",
    "    target_col = 'label'\n",
    "    n_fold = 5\n",
    "    trn_fold = [1,2,3,4,5]\n",
    "    \n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "def get_transforms(*, data):\n",
    "    if data == 'train':\n",
    "        return Compose([\n",
    "            RandomResizedCrop((CFG.size, CFG.size), scale=(0.4, 1.0)),\n",
    "            Transpose(p=0.5),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            VerticalFlip(p=0.5),\n",
    "            ShiftScaleRotate(p=0.5),\n",
    "            Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    \n",
    "    elif data == 'valid':\n",
    "       return Compose([\n",
    "           Resize(CFG.size, CFG.size),\n",
    "           Normalize(\n",
    "               mean=[0.485, 0.456, 0.406],\n",
    "               std=[0.229, 0.224, 0.225],\n",
    "           ),\n",
    "           ToTensorV2(),\n",
    "       ])\n",
    "        \n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.file_names = df['image_id'].values\n",
    "        self.labels = df['label'].values\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_names[idx]\n",
    "        file_path = f'{TRAIN_PATH}/{file_name}'\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        label = torch.tensor(self.labels[idx]).long()\n",
    "        return image, label\n",
    "\n",
    "# class TestDataset(Dataset):\n",
    "#     def __init__(self, df, transform=None):\n",
    "#         self.df = df\n",
    "#         self.file_names = df['image_id'].values\n",
    "#         self.labels = df['label'].values\n",
    "#         self.transform = transform\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.df)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         file_name = self.file_names[idx]\n",
    "#         file_path = f'{TRAIN_PATH}/{file_name}'\n",
    "#         image = cv2.imread(file_path)\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#         if self.transform:\n",
    "#             augmented = self.transform(image=image)\n",
    "#             image = augmented['image']\n",
    "#         label = torch.tensor(self.labels[idx]).long()\n",
    "#         return image, label\n",
    "\n",
    "class TestDataset2(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.file_names = df['image_path_id'].values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_names[idx]\n",
    "        image = cv2.imread(file_name)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image\n",
    "\n",
    "class CassavaDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.labels_df = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.labels = self.labels_df['label'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_dir, self.labels_df.iloc[idx, 0])\n",
    "        # image = Image.open(img_name)\n",
    "        # image = np.array(image)\n",
    "        image = cv2.imread(img_name)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        # label = self.labels_df.iloc[idx, 1]\n",
    "        label = torch.tensor(self.labels[idx]).long()\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.backbone = resnext50_32x4d(weights=None)\n",
    "        self.backbone.fc = nn.Linear(self.backbone.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.backbone(x)\n",
    "        return logits\n",
    "\n",
    "    def get_cam(self, x, class_idx=None):\n",
    "        features = []\n",
    "        gradients = []\n",
    "\n",
    "        def forward_hook(module, input, output):\n",
    "            features.append(output)\n",
    "\n",
    "        def backward_hook(module, grad_in, grad_out):\n",
    "            gradients.append(grad_out[0])\n",
    "\n",
    "        handle_f = self.backbone.layer4.register_forward_hook(forward_hook)\n",
    "        handle_b = self.backbone.layer4.register_full_backward_hook(backward_hook)\n",
    "\n",
    "        logits = self.forward(x)\n",
    "        if class_idx is None:\n",
    "            class_idx = logits.argmax(dim=1)\n",
    "\n",
    "        one_hot = torch.zeros_like(logits)\n",
    "        for i in range(logits.size(0)):\n",
    "            one_hot[i, class_idx[i]] = 1\n",
    "\n",
    "        self.zero_grad()\n",
    "        logits.backward(gradient=one_hot, retain_graph=True)\n",
    "\n",
    "        cams = []\n",
    "        for i in range(x.size(0)):\n",
    "            grad = gradients[0][i]\n",
    "            feat = features[0][i]\n",
    "            weights = grad.mean(dim=(1, 2))\n",
    "            cam = torch.sum(weights.view(-1, 1, 1) * feat, dim=0)\n",
    "            cam = F.relu(cam)\n",
    "            cam = cam - cam.min()\n",
    "            cam = cam / (cam.max() + 1e-8)\n",
    "            cams.append(cam.unsqueeze(0))\n",
    "\n",
    "        cam_tensor = torch.stack(cams)\n",
    "        cam_tensor = F.interpolate(cam_tensor, size=(x.size(2), x.size(3)), mode='bilinear', align_corners=False)\n",
    "        handle_f.remove()\n",
    "        handle_b.remove()\n",
    "        return cam_tensor.detach()\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super().__init__()\n",
    "        base = resnext50_32x4d(weights=None)\n",
    "        base.conv1 = nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            base.conv1,\n",
    "            base.bn1,\n",
    "            base.relu,\n",
    "            base.maxpool,\n",
    "            base.layer1,\n",
    "            base.layer2,\n",
    "            base.layer3,\n",
    "            base.layer4,\n",
    "            base.avgpool,\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        in_features = base.fc.in_features\n",
    "        self.fc_class = nn.Linear(in_features, num_classes)\n",
    "        # self.fc_judge = nn.Linear(in_features, 1)\n",
    "\n",
    "    def forward(self, image, cam):\n",
    "        x = torch.cat([image, cam], dim=1)\n",
    "        feat = self.feature_extractor(x)\n",
    "        feat = self.flatten(feat)\n",
    "        class_logits = self.fc_class(feat)\n",
    "        # judge_logits = self.fc_judge(feat)\n",
    "        # return class_logits, judge_logits\n",
    "        return class_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d1673db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:09:20.699813Z",
     "iopub.status.busy": "2025-05-19T07:09:20.699157Z",
     "iopub.status.idle": "2025-05-19T07:09:20.704050Z",
     "shell.execute_reply": "2025-05-19T07:09:20.703392Z"
    },
    "papermill": {
     "duration": 0.010143,
     "end_time": "2025-05-19T07:09:20.705668",
     "exception": false,
     "start_time": "2025-05-19T07:09:20.695525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomTarget:\n",
    "    def __init__(self, gt_class):\n",
    "        self.gt_class = gt_class\n",
    "\n",
    "    def __call__(self, model_output):\n",
    "        # model_output: [num_classes]\n",
    "        if self.gt_class == 4:\n",
    "            return model_output[4]\n",
    "        else:\n",
    "            return model_output[self.gt_class] - 0.6 * model_output[4]\n",
    "\n",
    "def remove_module_prefix(state_dict):\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        new_k = k.replace(\"backbone.\", \"\")\n",
    "        new_state_dict[new_k] = v\n",
    "    return new_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca5cb122",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:09:20.712531Z",
     "iopub.status.busy": "2025-05-19T07:09:20.712271Z",
     "iopub.status.idle": "2025-05-19T07:09:20.719086Z",
     "shell.execute_reply": "2025-05-19T07:09:20.718291Z"
    },
    "papermill": {
     "duration": 0.012034,
     "end_time": "2025-05-19T07:09:20.720758",
     "exception": false,
     "start_time": "2025-05-19T07:09:20.708724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class D1(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        img_path = os.path.join(self.img_dir, row[0])\n",
    "        # image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = cv2.imread(img_path)  # row[0] is the image file path\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        label = int(row[1])\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "\n",
    "        return idx, image, label\n",
    "\n",
    "val_transforms = Compose([\n",
    "           Resize(CFG.size, CFG.size),\n",
    "           Normalize(\n",
    "               mean=[0.485, 0.456, 0.406],\n",
    "               std=[0.229, 0.224, 0.225],\n",
    "           ),\n",
    "           ToTensorV2(),\n",
    "       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b6c5566",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:09:20.728033Z",
     "iopub.status.busy": "2025-05-19T07:09:20.727810Z",
     "iopub.status.idle": "2025-05-19T07:09:25.186573Z",
     "shell.execute_reply": "2025-05-19T07:09:25.185545Z"
    },
    "papermill": {
     "duration": 4.464749,
     "end_time": "2025-05-19T07:09:25.188537",
     "exception": false,
     "start_time": "2025-05-19T07:09:20.723788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24/1873441109.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(STAGE_ONE_MODEL)['model']\n",
      "/tmp/ipykernel_24/1873441109.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(STAGE_TWO_MODEL)['model']\n",
      "/tmp/ipykernel_24/526250068.py:12: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  img_path = os.path.join(self.img_dir, row[0])\n",
      "/tmp/ipykernel_24/526250068.py:16: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  label = int(row[1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # test_df = pd.read_csv(SUB_FILE)\n",
    "    \n",
    "    # # test_dataset = TestDataset(transform=get_transforms(data='valid'))\n",
    "    # train = pd.read_csv('../input/cassava-leaf-disease-classification/train.csv')\n",
    "    # # Split into folds for cross validation - we used the same split for all the models we trained!\n",
    "    # folds = train.merge(\n",
    "    #     pd.read_csv(FIVEFOLD)[[\"image_id\", \"fold\"]], on=\"image_id\")\n",
    "    # fold = 0\n",
    "    # trn_idx = folds[folds['fold'] != fold].index\n",
    "    # val_idx = folds[folds['fold'] == fold].index\n",
    "    # train_folds = folds.loc[trn_idx].reset_index(drop=True)\n",
    "    # valid_folds = folds.loc[val_idx].reset_index(drop=True)\n",
    "    # valid_dataset = CassavaDataset(valid_folds, transform=get_transforms(data='valid'))\n",
    "    # valid_loader = DataLoader(valid_dataset, batch_size=CFG.batch_size, \n",
    "    #                           shuffle=False, num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "    \n",
    "    # # load\n",
    "    # generator = resnext50_32x4d(weights=None).cuda()\n",
    "    # generator.fc = nn.Linear(generator.fc.in_features, 5)\n",
    "    # state_dict = torch.load(STAGE_ONE_MODEL)['model']\n",
    "    # state_dict = remove_module_prefix(state_dict)\n",
    "    # generator.load_state_dict(state_dict)\n",
    "    # generator = generator.cuda()\n",
    "    # discriminator = Discriminator().cuda()\n",
    "    # state_dict = torch.load(STAGE_TWO_MODEL)['model']\n",
    "    # discriminator.load_state_dict(state_dict)\n",
    "    # discriminator = discriminator.cuda()\n",
    "    \n",
    "    # criterion_class = nn.CrossEntropyLoss()\n",
    "    # criterion_judge = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # target_layers = [generator.layer4[-1]]\n",
    "    # cam = GradCAMPlusPlus(model=generator, target_layers=target_layers)\n",
    "\n",
    "    # generator.eval()\n",
    "    # discriminator.train()\n",
    "    # losses_d = AverageMeter()\n",
    "    # losses_d_class = AverageMeter()\n",
    "    # losses_d_judge = AverageMeter()\n",
    "    # # === Validation === (simulate validation set)\n",
    "    # losses_g = AverageMeter()\n",
    "    # losses_d = AverageMeter()\n",
    "    # generator.train()\n",
    "    # discriminator.eval()\n",
    "    # total_correct = 0\n",
    "    # total_d_correct = 0\n",
    "    # total_samples = 0\n",
    "\n",
    "    # predictions = []\n",
    "    # for step, (val_images, val_labels) in enumerate(valid_loader):\n",
    "    #     batch_size = val_labels.size(0)\n",
    "    #     val_images = val_images.cuda()\n",
    "    #     val_labels = val_labels.cuda()\n",
    "\n",
    "    #     # forward pass for generator with gradient tracking\n",
    "    #     val_logits = generator(val_images)\n",
    "    #     val_preds = val_logits.argmax(dim=1)\n",
    "    #     val_correct = (val_preds == val_labels)\n",
    "\n",
    "    #     val_cams = []\n",
    "    #     for i in range(batch_size):\n",
    "    #         input_tensor = val_images[i].unsqueeze(0)\n",
    "    #         gt = val_labels[i].item()\n",
    "    #         target = [CustomTarget(gt)]\n",
    "    #         grayscale_cam = cam(input_tensor=input_tensor, targets=target)[0]\n",
    "    #         val_cams.append(grayscale_cam)\n",
    "\n",
    "    #     val_cams = torch.from_numpy(np.stack(val_cams)).unsqueeze(1).float().cuda()\n",
    "  \n",
    "    #     with torch.no_grad():\n",
    "    #         val_g_loss = criterion_class(val_logits, val_labels)\n",
    "    #         val_d_class_logits = discriminator(val_images, val_cams)\n",
    "    #         val_d_class_loss = criterion_class(val_d_class_logits, val_labels)\n",
    "    #         val_d_preds = val_d_class_logits.argmax(dim=1)\n",
    "\n",
    "    #         predictions.extend(val_d_preds.cpu().numpy())  # Store predictions\n",
    "            \n",
    "    #         val_judge_labels = (val_preds == val_labels).float().unsqueeze(1)\n",
    "    #         val_d_loss = val_d_class_loss\n",
    "    #         val_d_correct = (val_d_preds == val_labels)\n",
    "\n",
    "    #     losses_g.update(val_g_loss.item(), batch_size)\n",
    "    #     losses_d.update(val_d_loss.item(), batch_size)\n",
    "    #     total_correct += val_correct.sum().item()\n",
    "    #     total_d_correct += val_d_correct.sum().item()\n",
    "    #     total_samples += batch_size\n",
    "    \n",
    "    # epoch_val_acc = total_correct / total_samples\n",
    "    # epoch_val_d_acc = total_d_correct / total_samples\n",
    "    # print(f\"Val G Accuracy: {epoch_val_acc:.4f} | Val D Accuracy: {epoch_val_d_acc:.4f}\")\n",
    "    # print(f\"Gen Loss: {losses_g.avg:.4f} | Dis Loss: {losses_d.avg:.4f}\")\n",
    "\n",
    "    # predictions = np.array(predictions)\n",
    "    # test_df['label'] = predictions\n",
    "    # test_df.to_csv('submission.csv', index=False)\n",
    "    # print(\"Submission file created!\")\n",
    "    ###########################################################################################\n",
    "    test_df = pd.read_csv(SUB_FILE)\n",
    "    \n",
    "    # test_dataset = TestDataset(transform=get_transforms(data='valid'))\n",
    "    # test_dataset = CassavaDataset(SUB_FILE, TEST_PATH, transform=get_transforms(data='valid'))\n",
    "    # predictions_resnext = pd.DataFrame(columns={\"image_id\"})\n",
    "    # predictions_resnext[\"image_id\"] = submission_df[\"image_id\"].values\n",
    "    # predictions_resnext['image_path_id'] = image_path + predictions_resnext['image_id'].astype(str)\n",
    "    # test_dataset = TestDataset2(predictions_resnext, transform=get_transforms(data='valid'))\n",
    "\n",
    "    # test_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, \n",
    "    #                           shuffle=False, num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    test_df = pd.read_csv('../input/cassava-leaf-disease-classification/sample_submission.csv')\n",
    "    test_dataset = D1('../input/cassava-leaf-disease-classification/sample_submission.csv',\n",
    "                      '../input/cassava-leaf-disease-classification/test_images',\n",
    "                      transform=val_transforms)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "    \n",
    "    # load\n",
    "    generator = resnext50_32x4d(weights=None).cuda()\n",
    "    generator.fc = nn.Linear(generator.fc.in_features, 5)\n",
    "    state_dict = torch.load(STAGE_ONE_MODEL)['model']\n",
    "    state_dict = remove_module_prefix(state_dict)\n",
    "    generator.load_state_dict(state_dict)\n",
    "    generator = generator.cuda()\n",
    "    discriminator = Discriminator().cuda()\n",
    "    state_dict = torch.load(STAGE_TWO_MODEL)['model']\n",
    "    discriminator.load_state_dict(state_dict)\n",
    "    discriminator = discriminator.cuda()\n",
    "    \n",
    "    criterion_class = nn.CrossEntropyLoss()\n",
    "    criterion_judge = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    target_layers = [generator.layer4[-1]]\n",
    "    cam = GradCAMPlusPlus(model=generator, target_layers=target_layers)\n",
    "\n",
    "    generator.eval()\n",
    "    discriminator.train()\n",
    "    # === Validation === (simulate validation set)\n",
    "    generator.train()\n",
    "    discriminator.eval()\n",
    "    total_correct = 0\n",
    "    total_d_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    predictions = []\n",
    "    # for step, (val_images, val_labels) in enumerate(test_loader):\n",
    "    for _, val_images, _ in test_loader:\n",
    "        batch_size = val_images.size(0)\n",
    "        val_images = val_images.cuda()\n",
    "\n",
    "        # forward pass for generator with gradient tracking\n",
    "        val_logits = generator(val_images)\n",
    "        val_preds = val_logits.argmax(dim=1)\n",
    "\n",
    "        val_cams = []\n",
    "        for i in range(batch_size):\n",
    "            input_tensor = val_images[i].unsqueeze(0)\n",
    "            target = [CustomTarget(val_preds[i].item())]\n",
    "            grayscale_cam = cam(input_tensor=input_tensor, targets=target)[0]\n",
    "            val_cams.append(grayscale_cam)\n",
    "\n",
    "        val_cams = torch.from_numpy(np.stack(val_cams)).unsqueeze(1).float().cuda()\n",
    "  \n",
    "        with torch.no_grad():\n",
    "            val_d_class_logits = discriminator(val_images, val_cams)\n",
    "            # val_d_preds = val_d_class_logits.argmax(dim=1)\n",
    "            _, val_d_preds = torch.max(val_d_class_logits, 1)\n",
    "            predictions.extend(val_d_preds.cpu().numpy())  # Store predictions\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    test_df['label'] = predictions\n",
    "    test_df.to_csv('submission.csv', index=False)\n",
    "    print(\"Submission file created!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 1718836,
     "sourceId": 13836,
     "sourceType": "competition"
    },
    {
     "datasetId": 7456201,
     "sourceId": 11865621,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7456313,
     "sourceId": 11865861,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7456678,
     "sourceId": 11866288,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 149959,
     "modelInstanceId": 127014,
     "sourceId": 149624,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 349347,
     "modelInstanceId": 328509,
     "sourceId": 401518,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 130.529682,
   "end_time": "2025-05-19T07:09:26.812748",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-19T07:07:16.283066",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
